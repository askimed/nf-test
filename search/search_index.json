{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#nf-test-a-simple-testing-framework-for-nextflow-pipelines","title":"nf-test: A simple testing framework for Nextflow pipelines","text":"<p>Test your production ready\u00a0Nextflow\u00a0pipelines in an efficient and automated way. \ud83d\ude80</p> <p> Getting Started  Installation  Source</p> <p> A DSL language similar to Nextflow  Describes expected behavior using 'when' and 'then' blocks   Abundance of functions for writing elegant and readable assertions  Utilizes snapshots to write tests for complex data structures   Provides commands for generating boilerplate code   Includes a test-runner that executes these scripts   Easy installation on CI systems </p>"},{"location":"#unit-testing","title":"Unit testing","text":"<p>nf-test enables you to test all components of your data science pipeline: from end-to-end testing of the entire pipeline to specific tests of processes or even custom functions. This ensures that all testing is conducted consistently across your project.</p>  Pipeline Process Functions <pre><code>nextflow_pipeline {\n\n  name \"Test Hello World\"\n  script \"nextflow-io/hello\"\n\n  test(\"hello world example should start 4 processes\") {\n    expect {\n      with(workflow) {\n        assert success\n        assert trace.tasks().size() == 4\n        assert \"Ciao world!\" in stdout\n        assert \"Bonjour world!\" in stdout\n        assert \"Hello world!\" in stdout\n        assert \"Hola world!\" in stdout\n      }\n    }\n  }\n\n}\n</code></pre> <pre><code>nextflow_process {\n\n    name \"Test Process SALMON_INDEX\"\n    script \"modules/local/salmon_index.nf\"\n    process \"SALMON_INDEX\"\n\n    test(\"Should create channel index files\") {\n\n        when {\n            process {\n                \"\"\"\n                input[0] = file(\"test_data/transcriptome.fa\")\n                \"\"\"\n            }\n        }\n\n        then {\n            //check if test case succeeded\n            assert process.success\n            //analyze trace file\n            assert process.trace.tasks().size() == 1\n            with(process.out) {\n                // check if emitted output has been created\n                assert index.size() == 1\n                // count amount of created files\n                assert path(index.get(0)).list().size() == 16\n                // parse info.json file\n                def info = path(index.get(0)+'/info.json').json\n                assert info.num_kmers == 375730\n                assert info.seq_length == 443050\n                //verify md5 checksum\n                assert path(index.get(0)+'/info.json').md5 == \"80831602e2ac825e3e63ba9df5d23505\"\n            }\n        }\n\n    }\n\n}\n</code></pre> <pre><code>nextflow_function {\n\n    name \"Test functions\"\n    script \"functions.nf\"\n\n    test(\"Test function1\") {\n      function \"function1\"\n      ...\n    }\n\n    test(\"Test function2\") {\n      function \"function2\"\n      ...\n    }\n}\n</code></pre> <p> Learn more about pipeline tests, workflow tests, process tests and function tests in the documentation.</p>"},{"location":"#snapshot-testing","title":"Snapshot testing","text":"<p>nf-test supports snapshot testing and automatically generates a baseline set of unit tests to safeguard against regressions caused by changes.nf-test captures a snapshot of output channels or any other objects and subsequently compares them to reference snapshot files stored alongside the tests. If the two snapshots do not match, the test will fail</p> <p> Learn more</p>"},{"location":"#highly-extendable","title":"Highly extendable","text":"<p>nf-test supports the inclusion of third-party libraries (e.g., jar files) or functions from Groovy files. This can be done to either extend its functionality or to prevent code duplication, thus maintaining simplicity in the logic of test cases. Given that many assertions are specific to use cases, nf-test incorporates a plugin system that allows for the extension of existing classes with custom methods. For example FASTA file support.</p> <p> Learn more</p>"},{"location":"#support-us","title":"Support us","text":"<p>We love stars as much as we love rockets! So make sure you star us on GitHub.</p> <p>Star</p> <p>Show the world your Nextflow pipeline is using nf-test and add the following badge to your <code>README.md</code>:</p> <p></p> <pre><code>[![nf-test](https://img.shields.io/badge/tested_with-nf--test-337ab7.svg)](https://code.askimed.com/nf-test)\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If you test your pipeline with nf-test, please cite:</p> <p>Forer, L., &amp; Sch\u00f6nherr, S. (2024). Improving the Reliability and Quality of Nextflow Pipelines with nf-test. bioRxiv. https://doi.org/10.1101/2024.05.25.595877</p>"},{"location":"#about","title":"About","text":"<p>nf-test has been created by Lukas Forer and Sebastian Sch\u00f6nherr and is MIT Licensed.</p> <p> </p> <p>Thanks to all the contributors to help us maintaining and improving nf-test!</p>"},{"location":"about/","title":"About","text":"<p>nf-test has been created by Lukas Forer and Sebastian Sch\u00f6nherr and is MIT Licensed.</p> <p> </p>"},{"location":"about/#contributors","title":"Contributors","text":"<p>Made with contrib.rocks.</p>"},{"location":"about/#statistics","title":"Statistics","text":"<p>GitHub:  </p> <p>Bioconda:  </p>"},{"location":"installation/","title":"Installation","text":"<p>nf-test has the same requirements as Nextflow and can be used on POSIX compatible systems like Linux or OS X. You can install nf-test using the following command:</p> <pre><code>curl -fsSL https://get.nf-test.com | bash\n</code></pre> <p>If you don't have curl installed, you could use wget:</p> <pre><code>wget -qO- https://get.nf-test.com | bash\n</code></pre> <p>It will create the <code>nf-test</code> executable file in the current directory. Optionally, move the <code>nf-test</code> file to a directory accessible by your <code>$PATH</code> variable.</p>"},{"location":"installation/#verify-installation","title":"Verify installation","text":"<p>Test the installation with the following command:</p> <pre><code>nf-test version\n</code></pre> <p>You should see something like this:</p> <pre><code>\ud83d\ude80 nf-test 0.5.0\nhttps://code.askimed.com/nf-test\n(c) 2021 -2022 Lukas Forer and Sebastian Schoenherr\n\nNextflow Runtime:\n\n      N E X T F L O W\n      version 21.10.6 build 5660\n      created 21-12-2021 16:55 UTC (17:55 CEST)\n      cite doi:10.1038/nbt.3820\n      http://nextflow.io\n</code></pre> <p>Now you are ready to write your first testcase.</p>"},{"location":"installation/#install-a-specific-version","title":"Install a specific version","text":"<p>If you want to install a specific version pass it to the install script as so</p> <pre><code>curl -fsSL https://get.nf-test.com | bash -s 0.7.0\n</code></pre>"},{"location":"installation/#manual-installation","title":"Manual installation","text":"<p>All releases are also available on Github.</p>"},{"location":"installation/#nextflow-binary-not-found","title":"Nextflow Binary not found?","text":"<p>If you get an error message like this, then nf-test was not able to detect your Nextflow installation.</p> <pre><code>\ud83d\ude80 nf-test 0.5.0\nhttps://code.askimed.com/nf-test\n(c) 2021 -2022 Lukas Forer and Sebastian Schoenherr\n\nNextflow Runtime:\nError: Nextflow Binary not found. Please check if Nextflow is in a directory accessible by your $PATH variable or set $NEXTFLOW_HOME.\n</code></pre> <p>To solve this issue you have two possibilites:</p> <ul> <li>Move your Nextflow binary to a directory accessible by your <code>$PATH</code> variable.</li> <li>Set the environment variable <code>NEXTFLOW_HOME</code> to the directory that contains the Nextflow binary.</li> </ul>"},{"location":"installation/#updating","title":"Updating","text":"<p>To update an existing nf-test installtion to the latest version, run the following command:</p> <pre><code>nf-test update\n</code></pre>"},{"location":"installation/#compiling-from-source","title":"Compiling from source","text":"<p>To compile nf-test from source you shall have maven installed. This will produce a <code>nf-test/target/nf-test.jar</code> file.</p> <p><pre><code>git clone git@github.com:askimed/nf-test.git\ncd nf-test\nmvn install\n</code></pre> To use the newly compiled <code>nf-test.jar</code>, update the <code>nf-test</code> bash script that is on your PATH to point to the new <code>.jar</code> file. First locate it with <code>which nf-test</code>, and then modify <code>APP_HOME</code> and <code>APP_JAR</code> vars at the top: <pre><code>#!/bin/bash\nAPP_HOME=\"/PATH/TO/nf-test/target/\"\nAPP_JAR=\"nf-test.jar\"\nAPP_UPDATE_URL=\"https://code.askimed.com/install/nf-test\"\n...\n</code></pre></p>"},{"location":"resources/","title":"Resources","text":"<p>This page collects videos and blog posts about nf-test created by the community. Have you authored a blog post or given a talk about nf-test? Feel free to contact us, and we will be delighted to feature it here.</p>"},{"location":"resources/#training-module-hello-nf-test","title":"Training Module: Hello nf-test","text":"<p>This training modules introduces nf-test in a simple and easy to follow-along hello-nextflow style aimed for beginners.</p> <p>hello nf-test training module</p>"},{"location":"resources/#blog-post-leveraging-nf-test-for-enhanced-quality-control-in-nf-core","title":"Blog post: Leveraging nf-test for enhanced quality control in nf-core","text":"<p>Reproducibility is an important attribute of all good science. This is specially true in the realm of bioinformatics, where software is hopefully being constantly updated, and pipelines are ideally being maintained. This blog post covers nf-test in the nf-core context.</p> <p>Read blog post</p>"},{"location":"resources/#nf-corebytesize-making-pipeline-level-tests-with-nf-test","title":"nf-core/bytesize: Making pipeline level tests with nf-test","text":"<p>In this nf-core bytesize session, Maxime Garcia shows how to make pipeline level tests with nf-test. If you like snapshots, CI/CD, and testing, this is for you!</p> <p>The presentation was recored as part of the nf-core/nft-utils series.</p>"},{"location":"resources/#nf-corebytesize-nft-bam","title":"nf-core/bytesize: nft-bam","text":"<p>Nicolas Vannieuwkerke presents nft-bam a nf-test plugin to get stable nf-test snapshots for SAM, BAM or CRAM files.</p> <p>The presentation was recored as part of the nf-core/nft-bam series.</p>"},{"location":"resources/#nf-corebytesize-converting-pytest-modules-to-nf-test","title":"nf-core/bytesize: Converting pytest modules to nf-test","text":"<p>Adam Talbot &amp; Sateesh Peri do a live demo of converting nf-core DSL2 modules pytests to nf-test</p> <p>The presentation was recored as part of the nf-core/bytesize series.</p>"},{"location":"resources/#nf-test-a-simple-but-powerful-testing-framework-for-nextflow-pipelines","title":"nf-test: a simple but powerful testing framework for Nextflow pipelines","text":"<p>Lukas Forer provides an overview of nf-test, its evolution over time and up-coming features.</p> <p>The presentation was recorded as part of the 2023 Nextflow Summit in Barcelona.</p>"},{"location":"resources/#nf-test-a-simple-test-framework-specifically-tailored-for-nextflow-pipelines","title":"nf-test, a simple test framework specifically tailored for Nextflow pipelines","text":"<p>Sateesh Peri does a hands-on exploration of nf-test, a simple test framework specifically tailored for Nextflow pipelines.</p> <p>Slides to follow along can be found here.</p> <p>The presentation was recorded as part of the Workflows Community Meetup - All Things Groovy at the Wellcome Genome Campus.</p>"},{"location":"resources/#nf-corebytesize-nf-test","title":"nf-core/bytesize: nf-test","text":"<p>Edmund Miller shares with us his impressions about nf-test from a user perspective. nf-test is a simple test framework for Nextflow pipelines.</p> <p>The presentation was recored as part of the nf-core/bytesize </p>"},{"location":"resources/#episode-8-nf-test-mentorships-and-debugging-resume","title":"Episode 8: nf-test, mentorships and debugging resume","text":"<p>Phil Ewels, Chris Hakkaart and Marcel Ribeiro-Dantas chat about the nf-test framework for testing Nextflow pipelines.</p> <p>The presentation was part of the \"News &amp; Views\" episode of Channels (Nextflow Podcast).</p>"},{"location":"resources/#blog-post-a-simple-test-framework-for-nextflow-pipelines","title":"Blog post: A simple test framework for Nextflow pipelines","text":"<p>Discover how nf-test originated from the need to efficiently and automatically test production-ready Nextflow pipelines.</p> <p>Read blog post</p>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#setup-nf-test-on-github-actions","title":"Setup nf-test on GitHub Actions","text":"<p>In this tutorial, we will guide you through setting up and running <code>nf-test</code> on GitHub Actions.</p> <p>Read tutorial</p>"},{"location":"docs/configuration/","title":"Configuration","text":""},{"location":"docs/configuration/#nf-testconfig","title":"<code>nf-test.config</code>","text":"<p>The <code>nf-test.config</code> file is a configuration file used to customize settings and behavior for <code>nf-test</code>. This file must be located in the root of your project, and it is automatically loaded when you run <code>nf-test test</code>. Below are the parameters that can be adapted:</p> Parameter Description Default Value <code>testsDir</code> Location for storing all nf-test cases (test scripts). If you want all test files to be in the same directory as the script itself, you can set the testDir to <code>.</code> <code>\"tests\"</code> <code>workDir</code> Directory for storing temporary files and working directories for each test. This directory should be added to <code>.gitignore</code>. <code>\".nf-test\"</code> <code>configFile</code> Location of an optional <code>nextflow.config</code> file specifically used for executing tests. Learn more. <code>\"tests/nextflow.config\"</code> <code>libDir</code> Location of a library folder that is automatically added to the classpath during testing to include additional libraries or resources needed for test cases. <code>\"tests/lib\"</code> <code>profile</code> Default profile to use for running tests defined in the Nextflow configuration. See Learn more. <code>\"docker\"</code> <code>withTrace</code> Enable or disable tracing options during testing. Disable tracing if your containers don't include the <code>procps</code> tool. <code>true</code> <code>autoSort</code> Enable or disable sorted channels by default when running tests. <code>true</code> <code>options</code> Custom Nextflow command-line options to be applied when running tests. For example <code>\"-dump-channels -stub-run\"</code> <code>ignore</code> List of filenames or patterns that should be ignored when building the dependency graph. For example: <code>ignore 'folder/**/*.nf', 'modules/module.nf'</code> `` <code>triggers</code> List of filenames or patterns that should be trigger a full test run. For example: <code>triggers 'nextflow.config', 'test-data/**/*'</code> `` <code>requires</code> Can be used to specify the minimum required version of nf-test. Requires nf-test &gt; 0.9.0 `` <p>Here's an example of what an <code>nf-test.config</code> file could look like:</p> <pre><code>config {\n    testsDir \"tests\"\n    workDir \".nf-test\"\n    configFile \"tests/nextflow.config\"\n    libDir \"tests/lib\"\n    profile \"docker\"\n    withTrace false\n    autoSort false\n    options \"-dump-channels -stub-run\"\n}\n</code></pre> <p>The <code>requires</code> keyword can be used to specify the minimum required version of nf-test. For instance, to ensure the use of at least nf-test version 0.9.0, define it as follows:</p> <pre><code>config {\n    requires (\n        \"nf-test\": \"0.9.0\"\n    )\n}\n</code></pre>"},{"location":"docs/configuration/#testsnextflowconfig","title":"<code>tests/nextflow.config</code>","text":"<p>This optional <code>nextflow.config</code> file is used to execute tests. This is a good place to set default <code>params</code> for all your tests. Example number of threads:</p> <pre><code>params {\n    // run all tests with 1 threads\n    threads = 1\n}\n</code></pre>"},{"location":"docs/configuration/#configuration-for-tests","title":"Configuration for tests","text":"<p>nf-test allows to set and overwrite the <code>config</code>, <code>autoSort</code> and <code>options</code> properties for a specific testsuite:</p> <pre><code>nextflow_process {\n\n    name \"Test Process...\"\n    script \"main.nf\"\n    process \"my_process\"\n    config \"path/to/test/nextflow.config\"\n    autoSort false\n    options \"-dump-channels\"\n    ...\n\n}\n</code></pre> <p>It is also possible to overwrite these properties for specific test. Depending on the used Nextflow option, also add the <code>--debug</code> nf-test option on the command-line to see the addtional output.</p> <pre><code>nextflow_process {\n\n   test(\"my test\") {\n\n      config \"path/to/test/nextflow.config\"\n      autoSort false\n      options \"-dump-channels\"\n      ...\n\n    }\n\n}\n</code></pre>"},{"location":"docs/configuration/#managing-profiles","title":"Managing Profiles","text":"<p>Profiles in <code>nf-test</code> provide a convenient way to configure and customize Nextflow executions for your test cases. To run your test using a specific Nextflow profile, you can use the <code>--profile</code> argument on the command line or define a default profile in <code>nf-test.config</code>.</p>"},{"location":"docs/configuration/#basic-profile-usage","title":"Basic Profile Usage","text":"<p>By default, <code>nf-test</code> reads the profile configuration from <code>nf-test.config</code>. If you've defined a profile called <code>A</code> in <code>nf-test.config</code>, running <code>nf-test --profile B</code> will start Nextflow with only the <code>B</code> profile. It replaces any existing profiles.</p>"},{"location":"docs/configuration/#combining-profiles-with","title":"Combining Profiles with \"+\"","text":"<p>To combine profiles, you can use the <code>+</code> prefix. For example, running <code>nf-test --profile +B</code> will start Nextflow with both <code>A</code> and <code>B</code> profiles, resulting in <code>-profile A,B</code>. This allows you to extend the existing configuration with additional profiles.</p>"},{"location":"docs/configuration/#profile-priority-order","title":"Profile Priority Order","text":"<p>Profiles are evaluated in a specific order, ensuring predictable behavior:</p> <ol> <li> <p>Profile in nf-test.config: The first profile considered is the one defined in <code>nf-test.config</code>.</p> </li> <li> <p>Profile Defined in Testcase: If you specify a profile within a testcase, it takes precedence over the one in <code>nf-test.config</code>.</p> </li> <li> <p>Profile Defined on the Command Line (CLI): Finally, any profiles provided directly through the CLI have the highest priority and override/extends previously defined profiles.</p> </li> </ol> <p>By understanding this profile evaluation order, you can effectively configure Nextflow executions for your test cases in a flexible and organized manner.</p>"},{"location":"docs/configuration/#file-staging","title":"File Staging","text":"<p>Warning</p> <p>File Staging is obsolete since version &gt;= 0.9.0.</p> <p>The <code>stage</code> section of the <code>nf-test.config</code> file is used to define files that are needed by Nextflow in the test environment (<code>meta</code> directory). Additionally, the directories <code>lib</code>, <code>bin</code>, and <code>assets</code> are automatically staged.</p>"},{"location":"docs/configuration/#supported-directives","title":"Supported Directives","text":""},{"location":"docs/configuration/#symlink","title":"<code>symlink</code>","text":"<p>This directive is used to create symbolic links (symlinks) in the test environment. Symlinks are pointers to files or directories and can be useful for creating references to data files or directories required for the test. The syntax for the <code>symlink</code> directive is as follows:</p> <pre><code>symlink \"source_path\"\n</code></pre> <p><code>source_path</code>: The path to the source file or directory that you want to symlink.</p>"},{"location":"docs/configuration/#copy","title":"<code>copy</code>","text":"<p>This directive is used to copy files or directories into the test environment. It allows you to duplicate files from a specified source to a location within the test environment. The syntax for the <code>copy</code> directive is as follows:</p> <pre><code>copy \"source_path\"\n</code></pre> <p><code>source_path</code>: The path to the source file or directory that you want to copy.</p>"},{"location":"docs/configuration/#example-usage","title":"Example Usage","text":"<p>Here's an example of how to use the <code>stage</code> section in an <code>nf-test.config</code> file:</p> <pre><code>config {\n    ...\n    stage {\n        symlink \"data/original_data.txt\"\n        copy \"resources/config.yml\"\n    }\n    ...\n}\n</code></pre> <p>In this example:</p> <ul> <li>The <code>symlink</code> directive creates a symlink named \"original_data.txt\" in the <code>meta</code> directory pointing to the file located at \"data/original_data.txt.\"</li> <li>The <code>copy</code> directive copies the \"config.yml\" file from the \"resources\" directory to the <code>meta</code> directory.</li> </ul>"},{"location":"docs/configuration/#testsuite","title":"Testsuite","text":"<p>Furthermore, it is also possible to stage files that are specific to a single testsuite:</p> <pre><code>nextflow_workflow {\n\n    name \"Test workflow HELLO_WORKFLOW\"\n\n    script \"./hello.nf\"\n    workflow \"HELLO_WORKFLOW\"\n\n    stage {\n        symlink \"test-assets/test.txt\"\n    }\n\n    test(\"Should print out test file\") {\n        expect {\n            assert workflow.success\n        }\n    }\n\n}\n</code></pre>"},{"location":"docs/getting-started/","title":"Getting started","text":"<p>This guide helps you to understand the concepts of nf-test and to write your first test cases. Before you start, please check if you have installed nf-test properly on your computer. Also, this guide assumes that you have a basic knowledge of Groovy and unit testing. The Groovy documentation is the best place to learn its syntax.</p>"},{"location":"docs/getting-started/#lets-get-started","title":"Let's get started","text":"<p>To show the power of nf-test, we adapted a recently published proof of concept Nextflow pipeline. We adapted the pipeline to the new DSL2 syntax using modules. First, open the terminal and clone our test pipeline:</p> <pre><code># clone nextflow pipeline\ngit clone https://github.com/askimed/nf-test-examples\n\n# enter project directory\ncd nf-test-examples\n</code></pre> <p>The pipeline consists of three modules (<code>salmon.index.nf</code>, <code>salmon_align_quant.nf</code>,<code>fastqc.nf</code>). Here, we use the <code>salmon.index.nf</code> process to create a test case from scratch. This process takes a reference as an input and creates an index using salmon.</p>"},{"location":"docs/getting-started/#init-new-project","title":"Init new project","text":"<p>Before creating test cases, we use the <code>init</code> command to setup nf-test.</p> <pre><code>//Init command has already been executed for our repository\nnf-test init\n</code></pre> <p>The <code>init</code> command creates the following files: <code>nf-test.config</code> and the <code>.nf-test/tests</code> folder.</p> <p>In the configuration section you can learn more about these files and how to customize the directory layout.</p>"},{"location":"docs/getting-started/#create-your-first-test","title":"Create your first test","text":"<p>The <code>generate</code> command helps you to create a skeleton test code for a Nextflow process or the complete pipeline/workflow.</p> <p>Here we generate a test case for the process <code>salmon.index.nf</code>:</p> <pre><code># delete already existing test case\nrm tests/modules/local/salmon_index.nf.test\nnf-test generate process modules/local/salmon_index.nf\n</code></pre> <p>This command creates a new file <code>tests/modules/local/salmon_index.nf</code> with the following content:</p> <pre><code>nextflow_process {\n\n    name \"Test Process SALMON_INDEX\"\n    script \"modules/local/salmon_index.nf\"\n    process \"SALMON_INDEX\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            params {\n                // define parameters here. Example:\n                // outdir = \"tests/results\"\n            }\n            process {\n                \"\"\"\n                // define inputs of the process here. Example:\n                // input[0] = file(\"test-file.txt\")\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            with(process.out) {\n              // Make assertions about the content and elements of output channels here. Example:\n              // assert out_channel != null\n            }\n        }\n\n    }\n\n}\n</code></pre> <p>The <code>generate</code> command filled automatically the name, script and process of our test case as well as created a skeleton for your first <code>test</code> method. Typically you create one file per process and use different <code>test</code> methods to describe the expected behaviour of the process.</p> <p>This <code>test</code> has a name, a <code>when</code> and a <code>then</code> closure (when/then closures are required here, since inputs need to be defined). The <code>when</code> block describes the input parameters of the workflow or the process. nf-test executes the process with exactly these parameters and parses the content of the output channels. Then, it evaluates the assertions defined in the <code>then</code> block to check if content of the output channels matches your expectations.</p>"},{"location":"docs/getting-started/#the-when-block","title":"The <code>when</code> block","text":"<p>The <code>when</code> block describes the input of the process and/or the Nextflow <code>params</code>.</p> <p>The <code>params</code> block is optional and is a simple map that can be used to override Nextflow's input <code>params</code>.</p> <p>The <code>process</code> block is a multi-line string. The <code>input</code> array can be used to set the different inputs arguments of the process. In our example, we only have one input that expects a file. Let us update the <code>process</code> block by setting the first element of the <code>input</code> array to the path of our reference file:</p> <pre><code>when {\n    params {\n        outdir = \"output\"\n    }\n    process {\n        \"\"\"\n        // Use transcriptome.fa as a first input paramter for our process\n        input[0] = file(\"${projectDir}/test_data/transcriptome.fa\")\n        \"\"\"\n    }\n}\n</code></pre> <p>Everything which is defined in the process block is later executed in a Nextflow script (created automatically to test your process). Therefore, you can use every Nextflow specific function or command to define the values of the input array (e.g. Channels, files, paths, etc.).</p>"},{"location":"docs/getting-started/#the-then-block","title":"The <code>then</code> block","text":"<p>The <code>then</code> block describes the expected output channels of the process when we execute it with the input parameters defined in the <code>when</code> block.</p> <p>The <code>then</code> block typically contains mainly assertions to check assumptions (e.g. the size and the content of an output channel). However, this block accepts every Groovy script. This means you can also import third party libraries to define very specific assertions.</p> <p>nf-test automatically loads all output channels of the process and all their items into a map named <code>process.out</code>. You can then use this map to formulate your assertions.</p> <p>For example, in the <code>salmon_index</code> process we expect to get one process executed and 16 files created. But we also want to check the md5 sum and want to look into the actual JSON file. Let us update the <code>then</code> section with some assertions that describe our expectations:</p> <pre><code>then {\n    //check if test case succeeded\n    assert process.success\n    //analyze trace file\n    assert process.trace.tasks().size() == 1\n    with(process.out) {\n      // check if emitted output has been created\n      assert index.size() == 1\n      // count amount of created files\n      assert path(index.get(0)).list().size() == 16\n      // parse info.json file using a json parser provided by nf-test\n      def info = path(index.get(0)+'/info.json').json\n      assert info.num_kmers == 375730\n      assert info.seq_length == 443050\n      assert path(index.get(0)+'/info.json').md5 == \"80831602e2ac825e3e63ba9df5d23505\"\n    }\n}\n</code></pre> <p>The items of a channel are always sorted by nf-test. This provides a deterministic order inside the channel and enables you to write reproducible tests.</p>"},{"location":"docs/getting-started/#your-first-test-specification","title":"Your first test specification","text":"<p>You can update the name of the test method to something that gives us later a good description of our specification. When we put everything together, we get the following full working test specification:</p> <pre><code>nextflow_process {\n\n    name \"Test Process SALMON_INDEX\"\n    script \"modules/local/salmon_index.nf\"\n    process \"SALMON_INDEX\"\n\n    test(\"Should create channel index files\") {\n\n        when {\n            process {\n                \"\"\"\n                input[0] = file(\"${projectDir}/test_data/transcriptome.fa\")\n                \"\"\"\n            }\n        }\n\n        then {\n            //check if test case succeeded\n            assert process.success\n            //analyze trace file\n            assert process.trace.tasks().size() == 1\n            with(process.out) {\n              // check if emitted output has been created\n              assert index.size() == 1\n              // count amount of created files\n              assert path(index.get(0)).list().size() == 16\n              // parse info.json file\n              def info = path(index.get(0)+'/info.json').json\n              assert info.num_kmers == 375730\n              assert info.seq_length == 443050\n              assert path(index.get(0)+'/info.json').md5 == \"80831602e2ac825e3e63ba9df5d23505\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"docs/getting-started/#run-your-first-test","title":"Run your first test","text":"<p>Now, the <code>test</code> command can be used to run your test:</p> <pre><code>nf-test test tests/modules/local/salmon_index.nf.test --profile docker\n</code></pre>"},{"location":"docs/getting-started/#specifying-profiles","title":"Specifying profiles","text":"<p>In this case, the <code>docker</code> profile defined in the Nextflow pipeline is used to execute the test. The profile is set using the <code>--profile</code> parameter, but you can also define a default profile in the configuration file.</p> <p>Congratulations! You created you first nf-test specification.</p>"},{"location":"docs/getting-started/#nextflow-options","title":"Nextflow options","text":"<p>nf-test also allows to specify Nextflow options (e.g. <code>-dump-channels</code>, <code>-stub-run</code>) globally in the nf-test.config file or by adding an option to the test suite or the actual test. Read more about this in the configuration documentation. </p> <pre><code>nextflow_process {\n\n    options \"-dump-channels\"\n\n}\n</code></pre>"},{"location":"docs/getting-started/#whats-next","title":"What's next?","text":"<ul> <li>Learn how to write assertions</li> <li>Learn how to write workflow tests (integration test or e2e)</li> <li>Learn how to config nf-test</li> </ul>"},{"location":"docs/nftest_pipelines/","title":"Pipelines using nf-test","text":""},{"location":"docs/nftest_pipelines/#nf-test-examples","title":"nf-test-examples","text":"<p>All test cases described in this documentation can be found in the nf-test-examples repository.</p>"},{"location":"docs/nftest_pipelines/#gwas-regenie-pipeline","title":"GWAS-Regenie Pipeline","text":"<p>To show the power of nf-test, we applied nf-test to a Nextflow pipeline that performs whole genome regression modelling using regenie. Please click here to learn more about this pipeline and checkout different kind of test cases.</p>"},{"location":"docs/running-tests/","title":"Running tests","text":""},{"location":"docs/running-tests/#basic-usage","title":"Basic usage","text":"<p>The easiest way to use nf-test is to run the following command. This command will run all tests under the <code>tests</code> directory. The <code>testDir</code> can be changed in the <code>nf-test.config</code>.</p> <pre><code>nf-test test\n</code></pre>"},{"location":"docs/running-tests/#execute-specific-tests","title":"Execute specific tests","text":"<p>You can also specify a list of tests, which should be executed. </p> <pre><code>nf-test test tests/modules/local/salmon_index.nf.test tests/modules/bwa_index.nf.test\n\nnf-test test tests/modules tests/modules/bwa_index.nf.test\n</code></pre>"},{"location":"docs/running-tests/#tag-tests","title":"Tag tests","text":"<p>nf-test provides a simple tagging mechanism that allows to execute tests by name or by tag.</p> <p>Tags can be defined for each testsuite or for each testcase using the new <code>tag</code> directive:</p> <pre><code>nextflow_process {\n\n    name \"suite 1\"\n    tag \"tag1\"\n\n    test(\"test 1\") {\n        tag \"tag2\"\n        tag \"tag3\"   \n        ...\n    }\n\n    test(\"test 2\") {\n\n        tag \"tag4\"\n        tag \"tag5\"   \n        ...\n\n    }\n}\n</code></pre> <p>For example, to execute all tests with <code>tag2</code> use the following command.</p> <pre><code>nf-test test --tag tag2  # collects test1\n</code></pre> <p>Names are automatically added to tags. This enables to execute suits or tests directly. </p> <pre><code>nf-test test --tag \"suite 1\"  # collects test1 and test2\n</code></pre> <p>When more tags are provided,\u00a0all tests that match at least one tag will be executed. Tags are also not case-sensitive, both lines will result the same tests.</p> <pre><code>nf-test test --tag tag3,tag4  # collects test1 and test2\nnf-test test --tag TAG3,TAG4  # collects test1 and test2\n</code></pre>"},{"location":"docs/running-tests/#create-a-tap-output","title":"Create a TAP output","text":"<p>To run all tests and create a <code>report.tap</code> file, use the following command.</p> <pre><code>nf-test test --tap report.tap\n</code></pre>"},{"location":"docs/running-tests/#run-test-by-its-hash-value","title":"Run test by its hash value","text":"<p>To run a specific test using its hash, the following command can be used. The hash value is generated during its first execution. </p> <pre><code>nf-test test tests/main.nf.test@d41119e4\n</code></pre>"},{"location":"docs/assertions/assertions/","title":"Assertions","text":"<p>Writing test cases means formulating assumptions by using assertions. Groovy\u2019s power assert provides a detailed output when the boolean expression validates to false. nf-test provides several extensions and commands to simplify the work with Nextflow channels. Here we summarise how nextflow and nf-test handles channels and provide examples for the tools that <code>nf-test</code> provides: </p> <ul> <li><code>with</code>: assert the contents of an item in a channel by index</li> <li><code>contains</code>: assert the contents of an item in the channel is present anywhere in the channel</li> <li><code>assertContainsInAnyOrder</code>: order-agnostic assertion of the contents of a channel</li> </ul>"},{"location":"docs/assertions/assertions/#nextflow-channels-and-nf-test-channel-sorting","title":"Nextflow channels and nf-test channel sorting","text":"<p>Nextflow channels emit (in a random order) a single value or a tuple of values. </p> <p>Channels that emit a single item produce an unordered list of objects, <code>List&lt;Object&gt;</code>, for example: <pre><code>process.out.outputCh = ['Hola', 'Hello', 'Bonjour']\n</code></pre></p> <p>Channels that contain Nextflow <code>file</code> values have a unique path each run. For Example: <pre><code>process.out.outputCh = ['/.nf-test/tests/c563c/work/65/85d0/Hola.json', '/.nf-test/tests/c563c/work/65/fa20/Hello.json', '/.nf-test/tests/c563c/work/65/b62f/Bonjour.json']\n</code></pre></p> <p>Channels that emit tuples produce an unordered list of ordered objects, <code>List&lt;List&lt;Object&gt;&gt;</code>: <pre><code>process.out.outputCh = [\n  ['Hola', '/.nf-test/tests/c563c/work/65/85d0/Hola.json'], \n  ['Hello', '/.nf-test/tests/c563c/work/65/fa20/Hello.json'], \n  ['Bonjour', '/.nf-test/tests/c563c/work/65/b62f/Bonjour.json']\n]\n</code></pre></p> <p>Assertions by channel index are made possible through sorting of the nextflow channel. The sorting is performed automatically by <code>nf-test</code> prior to launch of the <code>then</code> closure via integer, string and path comparisons. For example, the above would be sorted by <code>nf-test</code>: <pre><code>process.out.outputCh = [\n  ['Bonjour', '/.nf-test/tests/c563c/work/65/b62f/Bonjour.json'],\n  ['Hello', '/.nf-test/tests/c563c/work/65/fa20/Hello.json'],\n  ['Hola', '/.nf-test/tests/c563c/work/65/85d0/Hola.json']\n]\n</code></pre></p>"},{"location":"docs/assertions/assertions/#using-with","title":"Using <code>with</code>","text":"<p>This assertions...</p> <pre><code>assert process.out.imputed_plink2\nassert process.out.imputed_plink2.size() == 1\nassert process.out.imputed_plink2.get(0).get(0) == \"example.vcf\"\nassert process.out.imputed_plink2.get(0).get(1) ==~ \".*/example.vcf.pgen\"\nassert process.out.imputed_plink2.get(0).get(2) ==~ \".*/example.vcf.psam\"\nassert process.out.imputed_plink2.get(0).get(3) ==~ \".*/example.vcf.pvar\"\n</code></pre> <p>... can be written by using <code>with(){}</code> to improve readability:</p> <pre><code>assert process.out.imputed_plink2\nwith(process.out.imputed_plink2) {\n    assert size() == 1\n    with(get(0)) {\n        assert get(0) == \"example.vcf\"\n        assert get(1) ==~ \".*/example.vcf.pgen\"\n        assert get(2) ==~ \".*/example.vcf.psam\"\n        assert get(3) ==~ \".*/example.vcf.pvar\"\n    }\n}\n</code></pre>"},{"location":"docs/assertions/assertions/#using-contains-to-assert-an-item-in-the-channel-is-present","title":"Using <code>contains</code> to assert an item in the channel is present","text":"<p>Groovy's contains and collect methods can be used to flexibly assert an item exists in the channel output. </p> <p>For example, the below represents a channel that emits a two-element tuple, a string and a json file:  <pre><code>/*\ndef process.out.outputCh = [\n  ['Bonjour', '/.nf-test/tests/c563c/work/65/b62f/Bonjour.json'],\n  ['Hello', '/.nf-test/tests/c563c/work/65/fa20/Hello.json'],\n  ['Hola', '/.nf-test/tests/c563c/work/65/85d0/Hola.json']\n]\n*/\n</code></pre></p> <p>To assert the channel contains one of the tuples, parse the json and assert: <pre><code>testData = process.out.outputCh.collect { greeting, jsonPath -&gt; [greeting, path(jsonPath).json] } \nassert testData.contains(['Hello', path('./myTestData/Hello.json').json])\n</code></pre></p> <p>To assert a subset of the tuple data, filter the channel using collect. For example, to assert the greeting only: <pre><code>testData = process.out.outputCh.collect { greeting, jsonPath -&gt; greeting } \nassert testData.contains('Hello')\n</code></pre></p> <p>See the files page for more information on parsing and asserting various file types.</p>"},{"location":"docs/assertions/assertions/#using-assertcontainsinanyorder-for-order-agnostic-assertion-of-the-contents-of-a-channel","title":"Using <code>assertContainsInAnyOrder</code> for order-agnostic assertion of the contents of a channel","text":"<p><code>assertContainsInAnyOrder(List&lt;object&gt; list1, List&lt;object&gt; list2)</code> performs an order agnostic assertion on channels contents and is available in every <code>nf-test</code> closure. It is a binding for Hamcrest's assertContainsInAnyOrder.</p> <p>Some example use-cases are provided below.</p>"},{"location":"docs/assertions/assertions/#channel-that-emits-strings","title":"Channel that emits strings","text":"<pre><code>// process.out.outputCh = ['Bonjour', 'Hello', 'Hola'] \n\ndef expected = ['Hola', 'Hello', 'Bonjour']\nassertContainsInAnyOrder(process.out.outputCh, expected)\n</code></pre>"},{"location":"docs/assertions/assertions/#channel-that-emits-a-single-maps-eg-valmymap","title":"Channel that emits a single maps, e.g. val(myMap)","text":"<pre><code>/*\nprocess.out.outputCh = [\n  [\n    'D': [10,11,12],\n    'C': [7,8,9]\n  ],\n  [\n    'B': [4,5,6],\n    'A': [1,2,3]\n  ]\n]\n*/\n\ndef expected = [\n  [\n    'A': [1,2,3],\n    'B': [4,5,6]\n  ],\n  [\n    'C': [7,8,9],\n    'D': [10,11,12]\n  ]\n]\n\nassertContainsInAnyOrder(process.out.outputCh, expected)\n</code></pre>"},{"location":"docs/assertions/assertions/#channel-that-emits-json-files","title":"Channel that emits json files","text":"<p>See the files page for more information on parsing and asserting various file types.</p> <p>Since the outputCh filepaths are different between consecutive runs, the files need to be read/parsed prior to comparison</p> <pre><code>/*\nprocess.out.outputCh = [\n  '/.nf-test/tests/c563c/work/65/b62f/Bonjour.json',\n  '/.nf-test/tests/c563c/work/65/fa20/Hello.json',\n  '/.nf-test/tests/c563c/work/65/85d0/Hola.json'\n]\n*/\n\ndef actual = process.out.outputCh.collect { filepath -&gt; path(filepath).json }\ndef expected = [\n  path('./myTestData/Hello.json').json,\n  path('./myTestData/Hola.json').json,\n  path('./myTestData/Bonjour.json').json,\n]\n\nassertContainsInAnyOrder(actual, expected)\n</code></pre>"},{"location":"docs/assertions/assertions/#channel-that-emits-a-tuple-of-strings-and-json-files","title":"Channel that emits a tuple of strings and json files","text":"<p>See the files page for more information on parsing and asserting various file types.</p> <p>Since the ordering of items within the tuples are consistent, we can assert this case:</p> <pre><code>/*\nprocess.out.outputCh = [\n  ['Bonjour', '/.nf-test/tests/c563c/work/65/b62f/Bonjour.json'],\n  ['Hello', '/.nf-test/tests/c563c/work/65/fa20/Hello.json'],\n  ['Hola', '/.nf-test/tests/c563c/work/65/85d0/Hola.json']\n]\n*/\n\ndef actual = process.out.outputCh.collect { greeting, filepath -&gt; [greeting, path(filepath).json] }\ndef expected = [\n  ['Hola', path('./myTestData/Hola.json').json], \n  ['Hello', path('./myTestData/Hello.json').json],\n  ['Bonjour', path('./myTestData/Bonjour.json').json],\n]\n\nassertContainsInAnyOrder(actual, expected)\n</code></pre> <p>To assert the json only and ignore the strings: <pre><code>/*\nprocess.out.outputCh = [\n  ['Bonjour', '/.nf-test/tests/c563c/work/65/b62f/Bonjour.json'],\n  ['Hello', '/.nf-test/tests/c563c/work/65/fa20/Hello.json'],\n  ['Hola', '/.nf-test/tests/c563c/work/65/85d0/Hola.json']\n]\n*/\n\ndef actual = process.out.outputCh.collect { greeting, filepath -&gt; path(filepath).json }\ndef expected = [\n  path('./myTestData/Hello.json').json, \n  path('./myTestData/Hola.json').json,\n  path('./myTestData/Bonjour.json').json\n]\n\nassertContainsInAnyOrder(actual, expected)\n</code></pre></p> <p>To assert the strings only and not the json files: <pre><code>/*\nprocess.out.outputCh = [\n  ['Bonjour', '/.nf-test/tests/c563c/work/65/b62f/Bonjour.json'],\n  ['Hello', '/.nf-test/tests/c563c/work/65/fa20/Hello.json'],\n  ['Hola', '/.nf-test/tests/c563c/work/65/85d0/Hola.json']\n]\n*/\n\ndef actual = process.out.outputCh.collect { greeting, filepath -&gt; greeting }\ndef expected = ['Hello', 'Hola', 'Bonjour]\n\nassertContainsInAnyOrder(actual, expected)\n</code></pre></p>"},{"location":"docs/assertions/assertions/#using-assertall","title":"Using <code>assertAll</code>","text":"<p><code>assertAll(Closure... closures)</code> ensures that all supplied closures do no throw exceptions. The number of failed closures is reported in the Exception message. This useful for efficient debugging of a set of test assertions from a single test run.</p> <p><pre><code>def a = 2\n\nassertAll(\n    { assert a==1 },\n    { a = 1/0 },\n    { assert a==2 },\n    { assert a==3 }\n)\n</code></pre> The output will look like this: <pre><code>assert a==1\n       ||\n       |false\n       2\n\njava.lang.ArithmeticException: Division by zero\nAssertion failed:\n\nassert a==3\n       ||\n       |false\n       2\n\nFAILED (7.106s)\n\n  java.lang.Exception: 3 of 4 assertions failed\n</code></pre></p>"},{"location":"docs/assertions/fasta/","title":"FASTA Files","text":"<p> 0.7.0</p> <p>The nft-fasta plugin extends <code>path</code> by a <code>fasta</code> property that can be used to read FASTA files into maps. nft-fasta supports also gzipped FASTA files.</p>"},{"location":"docs/assertions/fasta/#setup","title":"Setup","text":"<p>To use the <code>fasta</code> property you need to activate the <code>nft-fasta</code> plugin in your <code>nf-test.config</code> file:</p> <pre><code>config {\n  plugins {\n    load \"nft-fasta@1.0.0\"\n  }\n}\n</code></pre> <p>More about plugins can be fond here.</p>"},{"location":"docs/assertions/fasta/#comparing-files","title":"Comparing files","text":"<pre><code>assert path('path/to/fasta1.fasta').fasta == path(\"path/to/fasta2.fasta'\").fasta\n</code></pre>"},{"location":"docs/assertions/fasta/#work-with-individual-samples","title":"Work with individual samples","text":"<pre><code>def sequences = path('path/to/fasta1.fasta.gz').fasta\nassert \"seq1\" in sequences\nassert !(\"seq8\" in sequences)\nassert sequences.seq1 == \"AGTACGTAGTAGCTGCTGCTACGTGCGCTAGCTAGTACGTCACGACGTAGATGCTAGCTGACTCGATGC\"\n</code></pre>"},{"location":"docs/assertions/files/","title":"Files","text":""},{"location":"docs/assertions/files/#md5-checksum","title":"md5 Checksum","text":"<p>nf-test extends <code>path</code> by a <code>md5</code> property that can be used to compare the file content with an expected checksum:</p> <p><pre><code>assert path(process.out.out_ch.get(0)).md5 == \"64debea5017a035ddc67c0b51fa84b16\"\n</code></pre> Note that for gzip compressed files, the <code>md5</code> property is calculated after gunzipping the file contents, whereas for other filetypes the <code>md5</code> property is directly calculated on the file itself.</p>"},{"location":"docs/assertions/files/#json-files","title":"JSON Files","text":"<p>nf-test supports comparison of JSON files and keys within JSON files. To assert that two JSON files contain the same keys and values: <pre><code>assert path(process.out.out_ch.get(0)).json == path('./some.json').json\n</code></pre> Individual keys can also be asserted:</p> <pre><code>assert path(process.out.out_ch.get(0)).json.key == \"value\"\n</code></pre>"},{"location":"docs/assertions/files/#yaml-files","title":"YAML Files","text":"<p>nf-test supports comparison of YAML files and keys within YAML files. To assert that two YAML files contain the same keys and values: <pre><code>assert path(process.out.out_ch.get(0)).yaml == path('./some.yaml').yaml\n</code></pre> Individual keys can also be asserted:</p> <pre><code>assert path(process.out.out_ch.get(0)).yaml.key == \"value\"\n</code></pre>"},{"location":"docs/assertions/files/#gzip-files","title":"GZip Files","text":"<p>nf-test extends <code>path</code> by a <code>linesGzip</code> property that can be used to read gzip compressed files.</p> <pre><code>assert path(process.out.out_ch.get(0)).linesGzip.size() == 5\nassert path(process.out.out_ch.get(0)).linesGzip.contains(\"Line Content\")\n</code></pre>"},{"location":"docs/assertions/files/#filter-lines","title":"Filter lines","text":"<p>The returned array can also be filtered by lines.</p> <pre><code>def lines = path(process.out.gzip.get(0)).linesGzip[0..5]\nassert lines.size() == 6\ndef lines = path(process.out.gzip.get(0)).linesGzip[0]\nassert lines.equals(\"MY_HEADER\")\n</code></pre>"},{"location":"docs/assertions/files/#grep-lines","title":"Grep lines","text":"<p>nf-test also provides the possibility to grep only specific lines with the advantage that only a subset of lines need to be read (especially helpful for larger files).</p> <pre><code>def lines = path(process.out.gzip.get(0)).grepLinesGzip(0,5)\nassert lines.size() == 6\ndef lines = path(process.out.gzip.get(0)).grepLineGzip(0)\nassert lines.equals(\"MY_HEADER\")\n</code></pre>"},{"location":"docs/assertions/files/#snapshot-support","title":"Snapshot Support","text":"<p>The possibility of filter lines from a *.gz file can also be combined with the snapshot functionality. </p> <pre><code>assert snapshot(\npath(process.out.gzip.get(0)).linesGzip[0]\n).match()\n</code></pre>"},{"location":"docs/assertions/libraries/","title":"Using Third-Party Libraries","text":"<p>nf-test supports including third party libraries (e.g. jar files ) or functions from groovy files to either extend it functionality or to avoid duplicate code and to keep the logic in test cases simple.</p>"},{"location":"docs/assertions/libraries/#using-local-groovy-files","title":"Using Local Groovy Files","text":"<p> 0.7.0 \u00b7</p> <p>If nf-test detects a <code>lib</code> folder in the directory of a tescase, then it adds it automatically to the classpath.</p>"},{"location":"docs/assertions/libraries/#examples","title":"Examples","text":"<p>We have a Groovy script <code>MyWordUtils.groovy</code> that contains the following class:</p> <pre><code>class MyWordUtils {\n\n    def static capitalize(String word){\n      return word.toUpperCase();\n    }\n\n}\n</code></pre> <p>We can put this file in a subfolder called <code>lib</code>:</p> <pre><code>testcase_1\n\u251c\u2500\u2500 capitalizer.nf\n\u251c\u2500\u2500 capitalizer.test\n\u2514\u2500\u2500 lib\n    \u2514\u2500\u2500 MyWordUtils.groovy\n</code></pre> <p>The file <code>capitalizer.nf</code> contains the <code>CAPITALIZER</code> process:</p> <pre><code>#!/usr/bin/env nextflow\nnextflow.enable.dsl=2\n\nprocess CAPITALIZER {\n    input:\n        val cheers\n    output:\n        stdout emit: output\n    script:\n       println \"$cheers\".toUpperCase()\n    \"\"\"\n    \"\"\"\n\n}\n</code></pre> <p>Next, we can use this class in the <code>capitalizer.nf.test</code> like every other class that is provided by nf-test or Groovy itself:</p> <pre><code>nextflow_process {\n\n    name \"Test Process CAPITALIZER\"\n    script \"capitalizer.nf\"\n    process \"CAPITALIZER\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            process {\n                \"\"\"\n                input[0] = \"world\"\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert process.stdout.contains(MyWordUtils.capitalize('world'))\n        }\n\n    }\n\n}\n</code></pre> <p>If we have a project and we want to reuse libraries in multiple test cases, then we can store the class in the shared lib folder. Both test cases are now able to use <code>MyWordUtils</code>:</p> <pre><code>tests\n\u251c\u2500\u2500 testcase_1\n    \u251c\u2500\u2500 hello_1.nf\n    \u251c\u2500\u2500 hello_1.nf.test\n\u251c\u2500\u2500 testcase_2\n    \u251c\u2500\u2500 hello_2.nf\n    \u251c\u2500\u2500 hello_2.nf.test\n\u2514\u2500\u2500 lib\n    \u2514\u2500\u2500 MyWordUtils.groovy\n</code></pre> <p>The default location is <code>tests/lib</code>. This folder location can be changed in nf-test config file.</p> <p>It is also possible to use the <code>--lib</code> parameter to add an additional folder to the classpath:</p> <pre><code>nf-test test tests/testcase_1/hello_1.nf.test --lib tests/mylibs\n</code></pre> <p>If multiple folders are used, the they need to be separate with a colon (like in Java or Groovy).</p>"},{"location":"docs/assertions/libraries/#using-local-jar-files","title":"Using Local Jar Files","text":"<p>To integrate local jar files, you can either specify the path to the jar within the nf-test <code>--lib</code> option </p> <pre><code>nf-test test test.nf.test --lib tests/lib/groovy-ngs-utils/groovy-ngs-utils.jar\n</code></pre> <p>or add it as follows to the <code>nf-test.config</code> file:</p> <pre><code>libDir \"tests/lib:tests/lib/groovy-ngs-utils/groovy-ngs-utils.jar\"\n</code></pre> <p>You could then import the class and use it in the <code>then</code> statement:</p> <pre><code>import gngs.VCF;\n\nnextflow_process {\n\n    name \"Test Process VARIANT_CALLER\"\n    script \"variant_caller.nf\"\n    process \"VARIANT_CALLER\"\n\n    test(\"Should run without failures\") {\n\n        when {\n           ...\n        }\n\n        then {\n            assert process.success             \n            def vcf = VCF.parse(\"$baseDir/tests/test_data/NA12879.vcf.gz\")\n            assert vcf.samples.size() == 10\n            assert vcf.variants.size() == 20\n        }\n\n    }\n\n}\n</code></pre>"},{"location":"docs/assertions/libraries/#using-maven-artifcats-with-grab","title":"Using Maven Artifcats with <code>@Grab</code>","text":"<p>nf-test supports the <code>@Grab</code> annotation to include third-party libraries that are available in a maven repository. As the dependency is defined as a maven artifact, there is no local copy of the jar file needed and maven enables to include an exact version as well as provides an easy update process.</p>"},{"location":"docs/assertions/libraries/#example","title":"Example","text":"<p>The following example uses the <code>WordUtil</code> class from <code>commons-lang</code>:</p> <pre><code>@Grab(group='commons-lang', module='commons-lang', version='2.4')\nimport org.apache.commons.lang.WordUtils\n\nnextflow_process {\n\n    name \"Test Process CAPITALIZER\"\n    script \"capitalizer.nf\"\n    process \"CAPITALIZER\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            process {\n                \"\"\"\n                input[0] = \"world\"\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert process.stdout.contains(WordUtils.capitalize('world'))\n        }\n\n    }\n\n}\n</code></pre>"},{"location":"docs/assertions/regular-expressions/","title":"Regular Expressions","text":""},{"location":"docs/assertions/regular-expressions/#using-operator","title":"Using <code>==~</code> operator","text":"<p>The operator <code>==~</code> can be used to check if a string matches a regular expression:</p> <pre><code>assert \"/my/full/path/to/process/dir/example.vcf.pgen\" ==~ \".*/example.vcf.pgen\"\n</code></pre>"},{"location":"docs/assertions/snapshots/","title":"Snapshots","text":"<p> 0.7.0</p> <p>Snapshots are a very useful tool whenever you want to make sure your output channels or output files not change unexpectedly. This feature is highly inspired by Jest.</p> <p>A typical snapshot test case takes a snapshot of the output channels or any other object, then compares it to a reference snapshot file stored alongside the test (<code>*.nf.test.snap</code>). The test will fail, if the two snapshots do not match: either the change is unexpected, or the reference snapshot needs to be updated to the new output of a process, workflow, pipeline or function.</p>"},{"location":"docs/assertions/snapshots/#using-snapshots","title":"Using Snapshots","text":"<p>The <code>snapshot</code> keyword creates a snapshot of the object and its <code>match</code> method can then be used to check if its contains the expected data from the snap file. The following example shows how to create a snapshot of a workflow channel:</p> <pre><code>assert snapshot(workflow.out.channel1).match()\n</code></pre> <p>You can also create a snapshot of all output channels of a process:</p> <pre><code>assert snapshot(process.out).match()\n</code></pre> <p>Or a specific check on a file:</p> <pre><code>assert snapshot(path(process.out.get(0))).match()\n</code></pre> <p>Even the result of a function can be used:</p> <pre><code>assert snapshot(function.result).match()\n</code></pre> <p>The first time this test runs, nf-test creates a snapshot file. This is a json file that contains a serialized version of the provided object.</p> <p>The snapshot file should be committed alongside code changes, and reviewed as part of your code review process. nf-test uses pretty-format to make snapshots human-readable during code review. On subsequent test runs, nf-test will compare the data with the previous snapshot. If they match, the test will pass. If they don't match, either the test runner found a bug in your code that should be fixed, or the implementation has changed and the snapshot needs to be updated.</p>"},{"location":"docs/assertions/snapshots/#updating-snapshots","title":"Updating Snapshots","text":"<p>When a snapshot test is failing due to an intentional implementation change, you can use the <code>--update-snapshot</code> flag to re-generate snapshots for all failed tests.</p> <pre><code>nf-test test tests/main.nf.test --update-snapshot\n</code></pre>"},{"location":"docs/assertions/snapshots/#cleaning-obsolete-snapshots","title":"Cleaning Obsolete Snapshots","text":"<p> 0.8.0</p> <p>Over time, snapshots can become outdated, leading to inconsistencies in your testing process. To help you manage obsolete snapshots, nf-test generates a list of these obsolete keys. This list provides transparency into which snapshots are no longer needed and can be safely removed.</p> <p>Running your tests with the <code>--clean-snapshot</code>or <code>--wipe-snapshot</code> option removes the obsolete snapshots from the snapshot file. This option is useful when you want to maintain the structure of your snapshot file but remove unused entries. It ensures that your snapshot file only contains the snapshots required for your current tests, reducing file bloat and improving test performance.</p> <pre><code>nf-test test tests/main.nf.test --clean-snapshot\n</code></pre> <p> Obsolete snapshots can only be detected when running all tests in a test file simultaneously, and when all tests pass. If you run a single test or if tests are skipped, nf-test cannot detect obsolete snapshots.</p>"},{"location":"docs/assertions/snapshots/#constructing-complex-snapshots","title":"Constructing Complex Snapshots","text":"<p>It is also possible to include multiple objects into one snapshot:</p> <pre><code>assert snapshot(workflow.out.channel1, workflow.out.channel2).match()\n</code></pre> <p>Every object that is serializable can be included into snapshots. Therefore you can even make a snapshot of the complete workflow or process object. This includes stdout, stderr, exist status, trace etc.  and is the easiest way to create a test that checks for all of this properties:</p> <pre><code>assert snapshot(workflow).match()\n</code></pre> <p>You can also include output files to a snapshot (e.g. useful in pipeline tests where no channels are available):</p> <pre><code>assert snapshot(\n    workflow,\n    path(\"${params.outdir}/file1.txt\"),\n    path(\"${params.outdir}/file2.txt\"),\n    path(\"${params.outdir}/file3.txt\")\n).match()\n</code></pre> <p>By default the snapshot has the same name as the test. You can also store a snapshot under a user defined name. This enables you to use multiple snapshots in one single test and to separate them in a logical way. In the following example a workflow snapshot is created, stored under the name \"workflow\".</p> <pre><code>assert snapshot(workflow).match(\"workflow\")\n</code></pre> <p>The next example creates a snapshot of two files and saves it under \"files\".</p> <pre><code>assert snapshot(path(\"${params.outdir}/file1.txt\"), path(\"${params.outdir}/file2.txt\")).match(\"files\")\n</code></pre> <p>You can also use helper methods to add objects to snapshots. For example, you can use the <code>list()</code>method to add all files of a folder to a snapshot:</p> <pre><code> assert snapshot(workflow, path(params.outdir).list()).match()\n</code></pre>"},{"location":"docs/assertions/snapshots/#compressed-snapshots","title":"Compressed Snapshots","text":"<p>If you add complex objects to snapshots with large content, you could use the <code>md5()</code> function to store the hashsum instead of the content in the snapshot file:</p> <pre><code> assert snapshot(hugeObject).md5().match()\n</code></pre>"},{"location":"docs/assertions/snapshots/#file-paths","title":"File Paths","text":"<p>If nf-test detects a path in the snapshot it automatically replace it by a unique fingerprint of the file that ensures the file content is the same. The fingerprint is default the md5 sum.</p>"},{"location":"docs/assertions/snapshots/#snapshot-differences","title":"Snapshot Differences","text":"<p> 0.8.0</p> <p>By default, nf-test uses the <code>diff</code> tool for comparing snapshots. It employs the following default arguments:</p> <ul> <li><code>-y</code>: Enables side-by-side comparison mode.</li> <li><code>-W 200</code>: Sets the maximum width for displaying the differences to 200 characters.</li> </ul> <p>These default arguments are applied when no custom settings are specified.</p> <p> If <code>diff</code>is not installed on the system, nf-test will print exepcted and found snapshots without highlighting differences.</p>"},{"location":"docs/assertions/snapshots/#customizing-diff-tool-arguments","title":"Customizing Diff Tool Arguments","text":"<p>Users have the flexibility to customize the arguments passed to the diff tool using an environment variable called <code>NFT_DIFF_ARGS</code>. This environment variable allows you to modify the way the diff tool behaves when comparing snapshots.</p> <p>To customize the arguments, follow these steps:</p> <ol> <li> <p>Set the <code>NFT_DIFF_ARGS</code> environment variable with your desired arguments.</p> <pre><code>export NFT_DIFF_ARGS=\"&lt;your_custom_arguments&gt;\"\n</code></pre> </li> <li> <p>Run <code>nf-test</code> to perform snapshot comparison, and it will utilize the custom arguments specified in <code>NFT_DIFF_ARGS</code>.</p> </li> </ol>"},{"location":"docs/assertions/snapshots/#changing-the-diff-tool","title":"Changing the Diff Tool","text":"<p><code>nf-test</code> not only allows you to customize the arguments but also provides the flexibility to change the diff tool itself. This can be achieved by using the environment variable <code>NFT_DIFF</code>.</p>"},{"location":"docs/assertions/snapshots/#example-using-icdiff","title":"Example: Using icdiff","text":"<p>As an example, you can change the diff tool to <code>icdiff</code>, which supports features like colors. To switch to <code>icdiff</code>, follow these steps:</p> <ol> <li> <p>Install icdiff</p> </li> <li> <p>Set the <code>NFT_DIFF</code> environment variable to <code>icdiff</code> to specify the new diff tool.</p> <pre><code>export NFT_DIFF=\"icdiff\"\n</code></pre> </li> <li> <p>If needed, customize the arguments for <code>icdiff</code> using <code>NFT_DIFF_ARGS</code> as explained in the previous section</p> <pre><code>export NFT_DIFF_ARGS=\"-N --cols 200 -L expected -L observed -t\"\n</code></pre> </li> <li> <p>Run <code>nf-test</code>, and it will use <code>icdiff</code> as the diff tool for comparing snapshots.</p> </li> </ol>"},{"location":"docs/cli/clean/","title":"<code>clean</code> command","text":""},{"location":"docs/cli/clean/#usage","title":"Usage","text":"<pre><code>nf-test clean\n</code></pre> <p>The <code>clean</code> command removes the <code>.nf-test</code> directory.</p>"},{"location":"docs/cli/generate/","title":"<code>generate</code> command","text":""},{"location":"docs/cli/generate/#usage","title":"Usage","text":"<pre><code>nf-test generate &lt;TEST_CASE_TYPE&gt; &lt;NEXTFLOW_FILES&gt;\n</code></pre>"},{"location":"docs/cli/generate/#supported-types","title":"Supported Types","text":""},{"location":"docs/cli/generate/#process","title":"<code>process</code>","text":""},{"location":"docs/cli/generate/#workflow","title":"<code>workflow</code>","text":""},{"location":"docs/cli/generate/#pipeline","title":"<code>pipeline</code>","text":""},{"location":"docs/cli/generate/#function","title":"<code>function</code>","text":""},{"location":"docs/cli/generate/#examples","title":"Examples","text":"<p>Create a test case for a process:</p> <pre><code>nf-test generate process modules/local/salmon_index.nf\n</code></pre> <p>Create a test cases for all processes in folder <code>modules</code>:</p> <pre><code>nf-test generate process modules/**/*.nf\n</code></pre> <p>Create a test case for a sub workflow:</p> <pre><code>nf-test generate workflow workflows/some_workflow.nf\n</code></pre> <p>Create a test case for the whole pipeline:</p> <pre><code>nf-test generate pipeline main.nf\n</code></pre> <p>Create a test case for each function in file <code>functions.nf</code>:</p> <pre><code>nf-test generate function functions.nf\n</code></pre>"},{"location":"docs/cli/init/","title":"<code>init</code> command","text":""},{"location":"docs/cli/init/#usage","title":"Usage","text":"<pre><code>nf-test init\n</code></pre> <p>The <code>init</code> command set ups nf-test in the current directory.</p> <p>The <code>init</code> command creates the following files: <code>nf-test.config</code> and <code>tests/nextflow.config</code>. It also creates a folder <code>tests</code> which is the home directory of your test code.</p> <p>In the configuration section you can learn more about these files and how to customize the directory layout.</p>"},{"location":"docs/cli/list/","title":"<code>list</code> command","text":""},{"location":"docs/cli/list/#usage","title":"Usage","text":"<p><code>list</code> command provides a convenient way to list all available test cases.</p> <pre><code>nf-test list [&lt;NEXTFLOW_FILES&gt;|&lt;SCRIPT_FOLDERS&gt;]\n</code></pre>"},{"location":"docs/cli/list/#optional-arguments","title":"Optional Arguments","text":""},{"location":"docs/cli/list/#-tags","title":"<code>--tags</code>","text":"<p>Print a list of all used tags.</p>"},{"location":"docs/cli/list/#-format-json","title":"<code>--format json</code>","text":"<p>Print the list of tests or tags as json object.</p>"},{"location":"docs/cli/list/#-format-raw","title":"<code>--format raw</code>","text":"<p>Print the list of tests or tags as simple list without formatting.</p>"},{"location":"docs/cli/list/#-silent","title":"<code>--silent</code>","text":"<p>Hide program version and header infos.</p>"},{"location":"docs/cli/list/#-debug","title":"<code>--debug</code>","text":"<p>Show debugging infos.</p>"},{"location":"docs/cli/list/#examples","title":"Examples","text":"<ul> <li> <p>List test cases that can be found in the <code>testDir</code> defined in the <code>nf-test.config</code> file in the current working directory:</p> <pre><code>nf-test list\n</code></pre> </li> <li> <p>List test cases in specified test scripts and search specified directories for additional test scripts:</p> <pre><code>nf-test list tests/modules/local/salmon_index.nf.test tests/modules/bwa_index.nf.test\n\nnf-test list tests/modules tests/modules/bwa_index.nf.test\n</code></pre> </li> <li> <p>List of all testcases as json:</p> </li> </ul> <pre><code>nf-test list --format json --silent\n[\"/Users/lukfor/Development/git/nf-gwas/tests/main.nf.test@69b98c67\",\"/Users/lukfor/Development/git/nf-gwas/tests/main.nf.test@fdb6c1cc\",\"/Users/lukfor/Development/git/nf-gwas/tests/main.nf.test@d1c219eb\",\"/Users/lukfor/Development/git/nf-gwas/tests/main.nf.test@3c54e3cb\",...]\n</code></pre> <ul> <li>List of all testcases as unformatted ist:</li> </ul> <pre><code>nf-test list --format raw --silent\n/Users/lukfor/Development/git/nf-gwas/tests/main.nf.test@69b98c67\n/Users/lukfor/Development/git/nf-gwas/tests/main.nf.test@fdb6c1cc\n/Users/lukfor/Development/git/nf-gwas/tests/main.nf.test@d1c219eb\n/Users/lukfor/Development/git/nf-gwas/tests/main.nf.test@3c54e3cb\n...\n</code></pre> <ul> <li>List of all tags as json:</li> </ul> <pre><code>nf-test list --tags --format json --silent\n[\"fastqc\",\"snakemake\"]\n</code></pre> <ul> <li>List of all tags as unformatted list:</li> </ul> <pre><code>nf-test list --tags --format raw --silent\nfastqc\nsnakemake\n</code></pre>"},{"location":"docs/cli/status/","title":"<code>status</code> command","text":"<p> 0.9.0</p>"},{"location":"docs/cli/status/#usage","title":"Usage","text":"<pre><code>nf-test status\n</code></pre> <p>The <code>status</code> command prints information about the number of Nextflow files that are covered by a test.</p>"},{"location":"docs/cli/status/#optional-arguments","title":"Optional Arguments","text":""},{"location":"docs/cli/status/#-csv-filename","title":"<code>--csv &lt;filename&gt;</code>","text":"<p>Writes a status report in csv format.</p>"},{"location":"docs/cli/status/#-html-filename","title":"<code>--html &lt;filename&gt;</code>","text":"<p>Writes a status report in html format.</p>"},{"location":"docs/cli/test/","title":"<code>test</code> command","text":""},{"location":"docs/cli/test/#usage","title":"Usage","text":"<pre><code>nf-test test [&lt;NEXTFLOW_FILES&gt;|&lt;SCRIPT_FOLDERS&gt;]\n</code></pre>"},{"location":"docs/cli/test/#optional-arguments","title":"Optional Arguments","text":""},{"location":"docs/cli/test/#-profile-nextflow_profile","title":"<code>--profile &lt;NEXTFLOW_PROFILE&gt;</code>","text":"<p>To run your test using a specific Nextflow profile, you can use the <code>--profile</code> argument. Learn more.</p>"},{"location":"docs/cli/test/#-dry-run","title":"<code>--dry-run</code>","text":"<p>This flag allows users to simulate the execution of tests.</p>"},{"location":"docs/cli/test/#-verbose","title":"<code>--verbose</code>","text":"<p>Prints out the Nextflow output during test runs.</p>"},{"location":"docs/cli/test/#-without-trace","title":"<code>--without-trace</code>","text":"<p>The Linux tool <code>procps</code> is required to run Nextflow tracing. In case your container does not support this tool, you can also run nf-test without tracing. Please note that the <code>workflow.trace</code> are not available when running it with this flag.</p>"},{"location":"docs/cli/test/#-tag-tag","title":"<code>--tag &lt;tag&gt;</code>","text":"<p>Execute only tests with the provided tag. Multiple tags can be used and have to be separated by commas (e.g. <code>tag1,tag2</code>).</p>"},{"location":"docs/cli/test/#-stop-on-first-failure","title":"<code>--stop-on-first-failure</code>","text":"<p>Stops execution after the first test failure.</p>"},{"location":"docs/cli/test/#-debug","title":"<code>--debug</code>","text":"<p>The debug parameter prints out debugging messages and all available output channels which can be accessed in the <code>then</code> clause.</p>"},{"location":"docs/cli/test/#output-reports","title":"Output Reports","text":""},{"location":"docs/cli/test/#-tap-filename","title":"<code>--tap &lt;filename&gt;</code>","text":"<p>Writes test results in TAP format to file.</p>"},{"location":"docs/cli/test/#-junitxml-filename","title":"<code>--junitxml &lt;filename&gt;</code>","text":"<p>Writes test results in JUnit XML format to file, which conforms to the standard schema.</p>"},{"location":"docs/cli/test/#-csv-filename","title":"<code>--csv &lt;filename&gt;</code>","text":"<p>Writes test results in csv file.</p>"},{"location":"docs/cli/test/#-ci","title":"<code>--ci</code>","text":"<p>By default,nf-test automatically stores a new snapshot. When CI mode is activated, nf-test will fail the test instead of storing the snapshot automatically.</p>"},{"location":"docs/cli/test/#-filter-types","title":"<code>--filter &lt;types&gt;</code>","text":"<p>Filter test cases by specified types (e.g., process, pipeline, workflow or function). Multiple types can be separated by commas.</p>"},{"location":"docs/cli/test/#optimizing-test-execution","title":"Optimizing Test Execution","text":""},{"location":"docs/cli/test/#-related-tests-files","title":"<code>--related-tests &lt;files&gt;</code>","text":"<p>Finds and executes all related tests for the provided .nf or nf.test files. Multiple files can be provided space separated.</p>"},{"location":"docs/cli/test/#-follow-dependencies","title":"<code>--follow-dependencies</code>","text":"<p>When this flag is set, nf-test will traverse all dependencies when the related-tests flag is set. This option is particularly useful when you need to ensure that all dependent tests are executed, bypassing the firewall calculation process.</p>"},{"location":"docs/cli/test/#-only-changed","title":"<code>--only-changed</code>","text":"<p>When enabled, this parameter instructs nf-test to execute tests only for files that have been modified within the current git working tree.</p>"},{"location":"docs/cli/test/#-changed-since-commit_hashbranch_name","title":"<code>--changed-since &lt;commit_hash|branch_name&gt;</code>","text":"<p>This parameter triggers the execution of tests related to changes made since the specifie commit. e.g. <code>--changed-since HEAD^</code> for all changes between the HEAD and HEAD - 1.</p>"},{"location":"docs/cli/test/#-changed-until-commit_hashbranch_name","title":"<code>--changed-until &lt;commit_hash|branch_name&gt;</code>","text":"<p>This parameter initiates the execution of tests related to changes made until the specified commit hash.</p>"},{"location":"docs/cli/test/#-graph-filename","title":"<code>--graph &lt;filename&gt;</code>","text":"<p>Enables the export of the dependency graph as a dot file. The dot file format is commonly used for representing graphs in graphviz and other related software.</p>"},{"location":"docs/cli/test/#sharding","title":"Sharding","text":"<p>This parameter allows users to divide the execution workload into manageable chunks, which can be useful for parallel or distributed processing.</p>"},{"location":"docs/cli/test/#-shard-shard","title":"<code>--shard &lt;shard&gt;</code>","text":"<p>Splits the execution into arbitrary chunks defined by the format <code>i/n</code>, where <code>i</code> denotes the index of the current chunk and <code>n</code> represents the total number of chunks. For instance, <code>2/5</code> executes the second chunk out of five.</p>"},{"location":"docs/cli/test/#-shard-strategy-strategy","title":"<code>--shard-strategy &lt;strategy&gt;</code>","text":"<p>Description: Specifies the strategy used to build shards when the <code>--shard</code> parameter is utilized. Accepted values are <code>round-robin</code> or <code>none.</code>. This parameter determines the method employed to distribute workload chunks among available resources. With the round-robin strategy, shards are distributed evenly among resources in a cyclic manner. The none strategy implies that shards won't be distributed automatically, and it's up to the user to manage the assignment of shards. Default value is <code>round-robin</code>.</p>"},{"location":"docs/cli/test/#examples","title":"Examples","text":"<ul> <li>Run all test scripts that can be found in the <code>testDir</code> defined in the <code>nf-test.config</code> file in the current working directory:</li> </ul> <pre><code>nf-test test\n</code></pre> <ul> <li>Run all specified test scripts and search specified directories for additional test scripts:</li> </ul> <pre><code>nf-test test tests/modules/local/salmon_index.nf.test tests/modules/bwa_index.nf.test\n\nnf-test test tests/modules tests/modules/bwa_index.nf.test\n</code></pre> <ul> <li>Run a specific test using its hash:</li> </ul> <pre><code>nf-test test tests/main.nf.test@d41119e4\n</code></pre> <ul> <li>Run all tests and write results to <code>report.tap</code>:</li> </ul> <pre><code>nf-test test --tap report.tap\n</code></pre> <ul> <li>Run all tests (and possible integration tests) for module <code>modules/module_a.nf</code> and <code>modules/module_b.nf</code>;</li> </ul> <pre><code>nf-test test --related-tests modules/module_a.nf modules/module_b.nf\n</code></pre> <ul> <li>If your project is a Git directory and you have modified files, you can run tests only for these changed files by using the following command:</li> </ul> <pre><code>nf-test test --only-changed\n</code></pre> <ul> <li>If you want to test all changes made between the current state of the repository and the last commit, you can use the following command:</li> </ul> <pre><code>nf-test test --changed-since HEAD^\n</code></pre> <ul> <li>Run only the second of four shards:</li> </ul> <pre><code>nf-test test --shard 2/4 \n</code></pre>"},{"location":"docs/plugins/developing-plugins/","title":"Plugin Development","text":"<p> 0.7.0</p> <p>The following plugin can be used as a boilerplate: https://github.com/askimed/nft-fasta</p> <p>The nf-test plugin cookiecutter template can also be used to set up the plugin base and immediately start developing.</p>"},{"location":"docs/plugins/developing-plugins/#developing-plugins","title":"Developing Plugins","text":"<p>A plugin has the possibility:</p> <ol> <li>Adding a new method to an existing class (e.g. the property <code>fasta</code> to class <code>Path</code>). It uses Groovy's ExtensionModule concept. Important: the method has to be static. One class can provide multiple methods.</li> </ol> <pre><code>// com.askimed.nf.test.fasta.PathExtension\npublic class PathExtension {\n  //can be used as: path(filename).fasta\n    public static Object getFasta(Path self) {\n    return FastaUtil.readAsMap(self);\n  }\n\n}\n</code></pre> <ol> <li>Providing new methods</li> </ol> <pre><code>// com.askimed.nf.test.fasta.Methods\npublic class Methods {\n\n  //can be used as: helloFasta()\n  public static void helloFasta() {\n    System.out.println(\"Hello FASTA\");\n  }\n\n}\n</code></pre>"},{"location":"docs/plugins/developing-plugins/#manifest-file","title":"Manifest file","text":"<p>You need to create a file <code>META-INF/nf-test-plugin</code> (in your resources). This file contains metadata about the plugin and both classes can now be registered by using the <code>extensionClasses</code> and <code>extensionMethods</code> properties.</p> <pre><code>moduleName=nft-my-plugin\nmoduleVersion=1.0.0\nmoduleAuthors=Lukas Forer\nextensionClasses=com.askimed.nf.test.fasta.PathExtension\nextensionMethods=com.askimed.nf.test.fasta.Methods\n</code></pre>"},{"location":"docs/plugins/developing-plugins/#building-a-jar-file","title":"Building a jar file","text":"<p>The plugin itself is a jar file that contains all classes and the <code>META-INF/nf-test-plugin</code> file. If you have dependencies then you have to create a uber-jar that includes all libraries, because nf-test doesn't support the classpath set in <code>META-INF\\MANIFEST</code>.</p>"},{"location":"docs/plugins/developing-plugins/#publishing-plugins","title":"Publishing Plugins","text":"<p>Available plugins are managed in this default repository: https://github.com/askimed/nf-test-plugins/blob/main/plugins.json</p> <p>Add your plugin or a new release to the <code>plugin.json</code> file and create a pull request to publish your plugin in the default repository. Or host you own repository:</p> <pre><code>[{\n  \"id\": \"nft-fasta\",\n  \"releases\": [{\n    \"version\": \"1.0.0\",\n    \"url\": \"https://github.com/askimed/nft-fasta/releases/download/v1.0.0/nft-fasta-1.0.0.jar\",\n  },{\n    \"version\": \"2.0.0\",\n    \"url\": \"https://github.com/askimed/nft-fasta/releases/download/v2.0.0/nft-fasta-2.0.0.jar\",\n  }]\n},{\n  \"id\": \"nft-my-plugin\",\n  \"releases\": [{\n    \"version\": \"1.0.0\",\n    \"url\": \"https://github.com/lukfor/nft-my-plugin2/releases/download/v1.0.0/nft-my-plugin-1.0.0.jar\",\n  }]\n}]\n</code></pre>"},{"location":"docs/plugins/using-plugins/","title":"Plugins","text":"<p> 0.7.0</p> <p>Most assertions are usecase specific. Therefore, separating this functionality and helper classes from the nf-test codebase has several advantages:</p> <ol> <li>nf-test releases are independent from plugin releases</li> <li>it is easier for third-parties to develop and maintain plugins</li> <li>it is possible to use private repositories to integrate private/protected code in plugins without sharing them</li> </ol> <p>For this purpose, we integrated the following plugin system that provides (a) the possibility to extend existing classes with custom methods (e.g. <code>path(filename).fasta</code>) and (2) to extends nf-test with new methods.</p>"},{"location":"docs/plugins/using-plugins/#using-plugins","title":"Using Plugins","text":"<p>Available plugins are listed here.</p> <p>A plugin can be activated via the <code>nf-test.config</code> by adding the <code>plugin</code> section and by using <code>load</code> method to specify the plugin and its version:</p> <pre><code>config {\n\n  plugins {\n\n    load \"nft-fasta@1.0.0\"\n\n  }\n\n}\n</code></pre> <p>It is also possible to add one ore more additional repositories. (Example: repository with development/snapshot versions, in-house repository, ...)</p> <pre><code>config {\n\n  plugins {\n\n    repository \"https://github.com/askimed/nf-test-plugins/blob/main/plugins-snapshots.json\"\n    repository \"https://github.com/seppinho/nf-test-plugin2/blob/main/plugins.json\"\n\n    load \"nft-fasta@1.1.0-snapshot\"\n    load \"nft-plugin2@1.1.0\"\n\n    // you can also load jar files directly without any repository\n    // loadFromFile \"path/to/my/nft-plugin.jar\"\n  }\n\n}\n</code></pre> <p>All plugins are downloaded and cached in <code>.nf-test\\plugins</code>. This installation mechanism is yet not safe for parallel execution when multiple nf-test instances are resolving the same plugin. However, you can use <code>nf-test update-plugins</code> to download all plugins before you run your tests in parallel.</p> <p>To clear the cache and to force redownloading plugins and repositories you can execute the <code>nf-test clean</code> command.</p> <p>One or multiple plugins can be activated also via the <code>--plugins</code> parameter:</p> <pre><code>nf-test test my-test.nf.test --plugins nft-fasta@1.0.0:plugin2@1.0.0\n</code></pre> <p>or</p> <pre><code>nf-test test my-test.nf.test --plugins path/to/my/nft-plugin.jar\n</code></pre>"},{"location":"docs/testcases/","title":"Documentation","text":""},{"location":"docs/testcases/global_variables/","title":"Global Variables","text":"<p>The following variables are available and can be used in <code>setup</code>, <code>when</code>, <code>then</code> and <code>cleanup</code> closures.</p> Name Description Example <code>baseDir</code> or<code>projectDir</code> The directory where the <code>nf-test.config</code> script is located. <code>mypipeline</code> <code>moduleDir</code> The directory where the module script is located. Also works for tesing subworkflows Modules: <code>mypipeline/modules/mymodule</code>  Subworkflows: <code>mypipeline/subworkflows/myworkflow</code> <code>moduleTestDir</code> The directory where the test script is located <code>mypipeline/tests/modules/mymodule</code> <code>launchDir</code> The directory where the test is run. <code>mypipeline/.nf-test/tests/&lt;test_hash&gt;</code> <code>metaDir</code> The directory where all meta are located (e.g. <code>mock.nf</code>). <code>mypipeline/.nf-test/tests/&lt;test_hash&gt;/meta</code> <code>workDir</code> The directory where tasks temporary files are created. <code>mypipeline/.nf-test/tests/&lt;test_hash&gt;/work</code> <code>outputDir</code> An output directory in the <code>$launchDir</code> that can be used to store output files. The variable contains the absolute path. If you need a relative outpu directory see <code>launchDir</code> example. <code>mypipeline/.nf-test/tests/&lt;test_hash&gt;/output</code> <code>params</code> Dictionary like object holding all parameters."},{"location":"docs/testcases/global_variables/#examples","title":"Examples","text":""},{"location":"docs/testcases/global_variables/#outputdir","title":"<code>outputDir</code>","text":"<p>This variable points to the directory within the temporary test directory (<code>.nf-test/tests/&lt;test-dir&gt;/output/</code>). The variable can be set under params:</p> <pre><code>params {\n    outdir = \"$outputDir\"\n}\n</code></pre>"},{"location":"docs/testcases/global_variables/#basedir","title":"<code>baseDir</code>","text":"<p>This variable points to the directory to locate the base directory of the main nf-test config. The variable can be used e.g. in the process definition to build absolute paths for input files:</p> <pre><code>process {\n    \"\"\"\n    file1 = file(\"$baseDir/tests/input/file123.gz\")\n    \"\"\"\n}\n</code></pre>"},{"location":"docs/testcases/global_variables/#launchdir","title":"<code>launchDir</code>","text":"<p>This variable points to the directory where the test is executed. This can be used get access to results that are created in an relative output directory:</p> <pre><code>when {\n    params {\n        outdir = \"results\"\n    }\n}\n</code></pre> <pre><code>then {\n    assert path(\"$launchDir/results\").exists()\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_function/","title":"Function Testing","text":"<p>nf-test allows testing of functions that are defined in a Nextflow file or defined in <code>lib</code>. Please checkout the CLI to generate a function test.</p>"},{"location":"docs/testcases/nextflow_function/#syntax","title":"Syntax","text":"<pre><code>nextflow_function {\n\n    name \"&lt;NAME&gt;\"\n    script \"&lt;PATH/TO/NEXTFLOW_SCRIPT.nf&gt;\"\n    function \"&lt;FUNCTION_NAME&gt;\"\n\n    test(\"&lt;TEST_NAME&gt;\") {\n\n    }\n}\n</code></pre> <p> Script paths that start with <code>./</code> or <code>../</code> are considered relative paths. These paths are resolved based on the location of the test script. Relative paths are beneficial when you want to reference files or directories located within the same directory as your test script or in a parent directory. These paths provide a convenient way to access files without specifying the entire path.</p>"},{"location":"docs/testcases/nextflow_function/#multiple-functions","title":"Multiple Functions","text":"<p>If a Nextflow script contains multiple functions and you want to test them all in the same testsuite, you can override the <code>function</code> property in each test. For example:</p>"},{"location":"docs/testcases/nextflow_function/#functionsnf","title":"<code>functions.nf</code>","text":"<pre><code>def function1() {\n  ...\n}\n\ndef function2() {\n  ...\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_function/#functionsnftest","title":"<code>functions.nf.test</code>","text":"<pre><code>nextflow_function {\n\n    name \"Test functions\"\n    script \"functions.nf\"\n\n    test(\"Test function1\") {\n      function \"function1\"\n      ...\n    }\n\n    test(\"Test function2\") {\n      function \"function2\"\n      ...\n    }\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_function/#functions-in-lib-folder","title":"Functions in <code>lib</code> folder","text":"<p>If you want to test a function that is inside a groovy file in your <code>lib</code> folder, you can ignore the <code>script</code> property, because Nextflow adds them automatically to the classpath. For example:</p>"},{"location":"docs/testcases/nextflow_function/#libutilsgroovy","title":"<code>lib\\Utils.groovy</code>","text":"<pre><code>class Utils {\n\n    public static void sayHello(name) {\n        if (name == null) {\n            error('Cannot greet a null person')\n        }\n\n        def greeting = \"Hello ${name}\"\n\n        println(greeting)\n    }\n\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_function/#testslibutilsgroovytest","title":"<code>tests\\lib\\Utils.groovy.test</code>","text":"<pre><code>nextflow_function {\n\n    name \"Test Utils.groovy\"\n\n    test(\"Test function1\") {\n      function \"Utils.sayHello\"\n      ...\n    }\n}\n</code></pre> <p>Note: the <code>generate function</code> command works only with Nextflow functions.</p>"},{"location":"docs/testcases/nextflow_function/#assertions","title":"Assertions","text":"<p>The <code>function</code> object can be used in asserts to check its status, result value or error messages.</p> <pre><code>// function status\nassert function.success\nassert function.failed\n\n// return value\nassert function.result == 27\n\n//returns a list containing all lines from stdout\nassert function.stdout.contains(\"Hello World\") == 3\n</code></pre>"},{"location":"docs/testcases/nextflow_function/#example","title":"Example","text":""},{"location":"docs/testcases/nextflow_function/#nextflow-script","title":"Nextflow script","text":"<p>Create a new file and name it <code>functions.nf</code>.</p> <pre><code>def say_hello(name) {\n    if (name == null) {\n        error('Cannot greet a null person')\n    }\n\n    def greeting = \"Hello ${name}\"\n\n    println(greeting)\n    return greeting\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_function/#nf-test-script","title":"nf-test script","text":"<p>Create a new file and name it <code>functions.nf.test</code>.</p> <pre><code>nextflow_function {\n\n  name \"Test Function Say Hello\"\n\n  script \"functions.nf\"\n  function \"say_hello\"\n\n  test(\"Passing case\") {\n\n    when {\n      function {\n        \"\"\"\n        input[0] = \"aaron\"\n        \"\"\"\n      }\n    }\n\n    then {\n      assert function.success\n      assert function.result == \"Hello aaron\"\n      assert function.stdout.contains(\"Hello aaron\")\n      assert function.stderr.isEmpty()\n    }\n\n  }\n\n  test(\"Failure Case\") {\n\n    when {\n      function {\n        \"\"\"\n        input[0] = null\n        \"\"\"\n      }\n    }\n\n    then {\n      assert function.failed\n      //It seems to me that error(..) writes message to stdout\n      assert function.stdout.contains(\"Cannot greet a null person\")\n    }\n  }\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_function/#execute-test","title":"Execute test","text":"<pre><code>nf-test test functions.nf.test\n</code></pre>"},{"location":"docs/testcases/nextflow_pipeline/","title":"Pipeline Testing","text":"<p>nf-test also allows to test the complete pipeline end-to-end. Please checkout the CLI to generate a pipeline test.</p>"},{"location":"docs/testcases/nextflow_pipeline/#syntax","title":"Syntax","text":"<pre><code>nextflow_pipeline {\n\n    name \"&lt;NAME&gt;\"\n    script \"&lt;PATH/TO/NEXTFLOW_SCRIPT.nf&gt;\"\n\n    test(\"&lt;TEST_NAME&gt;\") {\n\n    }\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_pipeline/#assertions","title":"Assertions","text":"<p>The <code>workflow</code> object can be used in asserts to check its status, error messages or traces.</p> <pre><code>// workflow status\nassert workflow.success\nassert workflow.failed\nassert workflow.exitStatus == 0\n\n// workflow error message\nassert workflow.errorReport.contains(\"....\")\n\n// trace\n//returns a list containing succeeded tasks\nassert workflow.trace.succeeded().size() == 3\n\n//returns a list containing failed tasks\nassert workflow.trace.failed().size() == 0\n\n//returns a list containing all tasks\nassert workflow.trace.tasks().size() == 3\n</code></pre>"},{"location":"docs/testcases/nextflow_pipeline/#example","title":"Example","text":""},{"location":"docs/testcases/nextflow_pipeline/#nextflow-script","title":"Nextflow script","text":"<p>Create a new file and name it <code>pipeline.nf</code>.</p> <pre><code>#!/usr/bin/env nextflow\nnextflow.enable.dsl=2\n\nprocess SAY_HELLO {\n    input:\n        val cheers\n\n    output:\n        stdout emit: verbiage_ch\n        path '*.txt', emit: verbiage_ch2\n\n    script:\n    \"\"\"\n    echo -n $cheers\n    echo -n $cheers &gt; ${cheers}.txt\n    \"\"\"\n}\n\nworkflow {\n    input = params.input_text.trim().split(',')\n    Channel.from(input) | SAY_HELLO\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_pipeline/#nf-test-script","title":"nf-test script","text":"<p>Create a new file and name it <code>pipeline.nf.test</code>.</p> <pre><code>nextflow_pipeline {\n\n    name \"Test Pipeline with 1 process\"\n    script \"pipeline.nf\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            params {\n              input_text = \"hello,nf-test\"\n            }\n        }\n\n        then {\n            assert workflow.success\n            assert workflow.trace.tasks().size() == 2\n        }\n\n    }\n\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_pipeline/#execute-test","title":"Execute test","text":"<pre><code>nf-test init\nnf-test test pipeline.nf.test\n</code></pre>"},{"location":"docs/testcases/nextflow_process/","title":"Process Testing","text":"<p>nf-test allows to test each process defined in a module file. Please checkout the CLI to generate a process test.</p>"},{"location":"docs/testcases/nextflow_process/#syntax","title":"Syntax","text":"<pre><code>nextflow_process {\n\n    name \"&lt;NAME&gt;\"\n    script \"&lt;PATH/TO/NEXTFLOW_SCRIPT.nf&gt;\"\n    process \"&lt;PROCESS_NAME&gt;\"\n\n    test(\"&lt;TEST_NAME&gt;\") {\n\n    }\n}\n</code></pre> <p> Script paths that start with <code>./</code> or <code>../</code> are considered relative paths. These paths are resolved based on the location of the test script. Relative paths are beneficial when you want to reference files or directories located within the same directory as your test script or in a parent directory. These paths provide a convenient way to access files without specifying the entire path.</p>"},{"location":"docs/testcases/nextflow_process/#assertions","title":"Assertions","text":"<p>The <code>process</code> object can be used in asserts to check its status or error messages.</p> <pre><code>// process status\nassert process.success\nassert process.failed\nassert process.exitStatus == 0\n\n// Analyze Nextflow trace file\nassert process.trace.tasks().size() == 1\n\n// process error message\nassert process.errorReport.contains(\"....\")\n\n//returns a list containing all lines from stdout\nassert process.stdout.contains(\"Hello World\") == 3\n</code></pre>"},{"location":"docs/testcases/nextflow_process/#output-channels","title":"Output Channels","text":"<p>The <code>process.out</code> object provides access to the content of all named output Channels (see Nextflow <code>emit</code>):</p> <pre><code>// channel exists\nassert process.out.my_channel != null\n\n// channel contains 3 elements\nassert process.out.my_channel.size() == 3\n\n// first element is \"hello\"\nassert process.out.my_channel.get(0) == \"hello\"\n</code></pre> <p>Channels that lack explicit names can be addressed using square brackets and the corresponding index. This indexing method provides a straightforward way to interact with channels without the need for predefined names. To access the first output channel, you can use the index [0] as demonstrated below:</p> <pre><code>// channel exists\nassert process.out[0] != null\n\n// channel contains 3 elements\nassert process.out[0].size() == 3\n\n// first element is \"hello\"\nassert process.out[0].get(0) == \"hello\"\n</code></pre>"},{"location":"docs/testcases/nextflow_process/#example","title":"Example","text":""},{"location":"docs/testcases/nextflow_process/#nextflow-script","title":"Nextflow script","text":"<p>Create a new file and name it <code>say_hello.nf</code>.</p> <pre><code>#!/usr/bin/env nextflow\nnextflow.enable.dsl=2\n\nprocess SAY_HELLO {\n    input:\n        val cheers\n\n    output:\n        stdout emit: verbiage_ch\n        path '*.txt', emit: verbiage_ch2\n\n    script:\n    \"\"\"\n    echo -n $cheers\n    echo -n $cheers &gt; ${cheers}.txt\n    \"\"\"\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_process/#nf-test-script","title":"nf-test script","text":"<p>Create a new file and name it <code>say_hello.nf.test</code>.</p> <pre><code>nextflow_process {\n\n    name \"Test Process SAY_HELLO\"\n    script \"say_hello.nf\"\n    process \"SAY_HELLO\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            process {\n                \"\"\"\n                input[0] = Channel.from('hello','nf-test')\n                \"\"\"\n            }\n        }\n\n        then {\n\n            assert process.success\n            assert process.trace.tasks().size() == 2\n\n            with(process.out.verbiage_ch2) {\n                assert size() == 2\n                assert path(get(0)).readLines().size() == 1\n                assert path(get(1)).readLines().size() == 1\n                assert path(get(1)).md5 == \"4a17df7a54b41a84df492da3f1bab1e3\"\n            }\n\n        }\n\n    }\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_process/#execute-test","title":"Execute test","text":"<pre><code>nf-test init\nnf-test test say_hello.nf.test\n</code></pre>"},{"location":"docs/testcases/nextflow_workflow/","title":"Workflow Testing","text":"<p>nf-test also allows to test a specific workflow. Please checkout the CLI to generate a workflow test.</p>"},{"location":"docs/testcases/nextflow_workflow/#syntax","title":"Syntax","text":"<pre><code>nextflow_workflow {\n\n    name \"&lt;NAME&gt;\"\n    script \"&lt;PATH/TO/NEXTFLOW_SCRIPT.nf&gt;\"\n    workflow \"&lt;WORKFLOW_NAME&gt;\"\n\n    test(\"&lt;TEST_NAME&gt;\") {\n\n    }\n}\n</code></pre> <p> Script paths that start with <code>./</code> or <code>../</code> are considered relative paths. These paths are resolved based on the location of the test script. Relative paths are beneficial when you want to reference files or directories located within the same directory as your test script or in a parent directory. These paths provide a convenient way to access files without specifying the entire path.</p>"},{"location":"docs/testcases/nextflow_workflow/#assertions","title":"Assertions","text":"<p>The <code>workflow</code> object can be used in asserts to check its status, error messages or traces.</p> <pre><code>// workflow status\nassert workflow.success\nassert workflow.failed\nassert workflow.exitStatus == 0\n\n// workflow error message\nassert workflow.errorReport.contains(\"....\")\n\n// trace\n//returns a list containing succeeded tasks\nassert workflow.trace.succeeded().size() == 3\n\n//returns a list containing failed tasks\nassert workflow.trace.failed().size() == 0\n\n//returns a list containing all tasks\nassert workflow.trace.tasks().size() == 3\n\n//returns a list containing all lines from stdout\nassert workflow.stdout.contains(\"Hello World\") == 3\n</code></pre>"},{"location":"docs/testcases/nextflow_workflow/#output-channels","title":"Output Channels","text":"<p>The <code>workflow.out</code> object provides access to the content of all named output Channels (see Nextflow <code>emit</code>):</p> <pre><code>// channel exists\nassert workflow.out.my_channel != null\n\n// channel contains 3 elements\nassert workflow.out.my_channel.size() == 3\n\n// first element is \"hello\"\nassert workflow.out.my_channel.get(0) == \"hello\"\n</code></pre>"},{"location":"docs/testcases/nextflow_workflow/#example","title":"Example","text":""},{"location":"docs/testcases/nextflow_workflow/#nextflow-script","title":"Nextflow script","text":"<p>Create a new file and name it <code>trial.nf</code>.</p> <pre><code>#!/usr/bin/env nextflow\nnextflow.enable.dsl=2\n\nprocess sayHello {\n    input:\n        val cheers\n\n    output:\n        stdout emit: verbiage_ch\n        path '*.txt', emit: verbiage_ch2\n\n    script:\n    \"\"\"\n    echo -n $cheers\n    echo -n $cheers &gt; ${cheers}.txt\n    \"\"\"\n}\n\nworkflow trial {\n    take: things\n    main:\n        sayHello(things)\n        sayHello.out.verbiage_ch.view()\n    emit:\n        trial_out_ch = sayHello.out.verbiage_ch2\n}\n\nworkflow {\n    Channel.from('hello','nf-test') | trial\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_workflow/#nf-test-script","title":"nf-test script","text":"<p>Create a new file and name it <code>trial.nf.test</code>.</p> <pre><code>nextflow_workflow {\n\n    name \"Test Workflow Trial\"\n    script \"trial.nf\"\n    workflow \"trial\"\n\n    test(\"Should run without failures\") {\n\n        when {\n            workflow {\n                \"\"\"\n                input[0] = Channel.from('hello','nf-test')\n                \"\"\"\n            }\n        }\n\n        then {\n\n            assert workflow.success\n\n            with(workflow.out.trial_out_ch) {\n                assert size() == 2\n                assert path(get(0)).readLines().size() == 1\n                assert path(get(1)).readLines().size() == 1\n                assert path(get(1)).md5 == \"4a17df7a54b41a84df492da3f1bab1e3\"\n            }\n\n        }\n\n    }\n\n}\n</code></pre>"},{"location":"docs/testcases/nextflow_workflow/#execute-test","title":"Execute test","text":"<pre><code>nf-test init\nnf-test test trial.nf.test\n</code></pre>"},{"location":"docs/testcases/params/","title":"Params Dictionary","text":"<p>The <code>params</code> block is optional and is a simple map that can be used to overwrite Nextflow's input <code>params</code>. The <code>params</code> block is located in the <code>when</code> block of a testcase. You can set params manually:</p> <pre><code>when {\n    params {\n        outdir = \"output\"\n    }\n}\n</code></pre> <p>It is also possible to set nested params using the same syntax as in your Nextflow script:</p> <pre><code>when {\n    params {\n        output {\n          dir = \"output\"\n        }\n    }\n}\n</code></pre> <p>The <code>params</code> map can also be used in the <code>then</code> block:</p> <pre><code>then {\n    assert params.output == \"output\"    \n}\n</code></pre>"},{"location":"docs/testcases/params/#load-params-from-files","title":"Load params from files","text":"<p>In addition, you can load the <code>params</code> from a JSON file:</p> <pre><code>when {\n    params {\n        load(\"$baseDir/tests/params.json\")\n    }\n}\n</code></pre> <p>or from a YAML file:</p> <pre><code>when {\n    params {\n        load(\"$baseDir/tests/params.yaml\")\n    }\n}\n</code></pre> <p>nf-test allows to combine both techniques and therefor it is possible to overwrite one or more <code>params</code> from the json file:</p> <pre><code>when {\n    params {\n        load(\"$baseDir/tests/params.json\")\n        outputDir = \"new/output/path\"\n    }\n}\n</code></pre>"},{"location":"docs/testcases/setup/","title":"Setup Method","text":"<p>The setup method allows you to specify processes or workflows that need to be executed before the primary <code>when</code> block. It serves as a mechanism to prepare the required input data or set up essential steps prior to the primary processing block.</p>"},{"location":"docs/testcases/setup/#syntax","title":"Syntax","text":"<p>The setup method is typically used within the context of a test case. The basic syntax for the setup method is as follows:</p> <pre><code>test(\"my test\"){\n    setup {\n        // Define and execute dependent processes or workflows here\n    }\n}\n</code></pre> <p>Within the setup block, you can use the <code>run</code> method to define and execute dependent processes or workflows.</p> <p>The <code>run</code> method syntax for a process is as follows:</p> <pre><code>run(\"ProcessName\") {\n    script \"path/to/process/script.nf\"\n    process {\n        // Define the process inputs here\n    }\n}\n</code></pre> <p>The <code>run</code> method syntax for a workflow is as follows:</p> <pre><code>run(\"WorkflowName\") {\n    script \"path/to/workflow/script.nf\"\n    workflow {\n        // Define the workflow inputs here\n    }\n}\n</code></pre> <p>If you need to run the same process multiple times, you can set the alias of the process:</p> <pre><code>run(\"GENERATE_DATA\", alias: \"MY_PROCESS\") {\n    script \"./generate_data.nf\"\n    process {\n       ...\n    }\n}\n</code></pre> <p>Warning</p> <p>Please keep in mind that changes in procsses or workflows, which are executed in the setup method, can result in a failed test run.</p>"},{"location":"docs/testcases/setup/#example-usage","title":"Example Usage","text":""},{"location":"docs/testcases/setup/#1-local-setup-method","title":"1. Local Setup Method","text":"<p>In this example, we create a setup method within a Nextflow process definition to execute a dependent process named \"ABRICATE_RUN.\" This process generates input data that is required for the primary process \"ABRICATE_SUMMARY.\" The <code>setup</code> block specifies the execution of \"ABRICATE_RUN,\" and the <code>when</code> block defines the processing logic for \"ABRICATE_SUMMARY.\"</p> <pre><code>nextflow_process {\n\n    name \"Test process data\"\n\n    script \"../main.nf\"\n    process \"ABRICATE_SUMMARY\"\n    config \"./nextflow.config\"\n\n    test(\"Should use process ABRICATE_RUN to generate input data\") {\n\n        setup {\n\n            run(\"ABRICATE_RUN\") {\n                script \"../../run/main.nf\"\n                process {\n                    \"\"\"\n                    input[0] =  Channel.fromList([\n                        tuple([ id:'test1', single_end:false ], // meta map\n                            file(params.test_data['bacteroides_fragilis']['genome']['genome_fna_gz'], checkIfExists: true)),\n                        tuple([ id:'test2', single_end:false ],\n                            file(params.test_data['haemophilus_influenzae']['genome']['genome_fna_gz'], checkIfExists: true))\n                    ])\n                    \"\"\"\n                }\n            }\n\n        }\n\n        when {\n            process {\n                \"\"\"\n                input[0] = ABRICATE_RUN.out.report.collect{ meta, report -&gt; report }.map{ report -&gt; [[ id: 'test_summary'], report]}\n                \"\"\"\n            }\n        }\n\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n    }\n\n}\n</code></pre>"},{"location":"docs/testcases/setup/#2-global-setup-method","title":"2. Global Setup Method","text":"<p>In this example, a global setup method is defined for all tests within a Nextflow process definition. The setup method is applied to multiple test cases, ensuring consistent setup for each test. This approach is useful when multiple tests share the same setup requirements.</p> <pre><code>nextflow_process {\n\n    name \"Test process data\"\n\n    script \"../main.nf\"\n    process \"ABRICATE_SUMMARY\"\n    config \"./nextflow.config\"\n\n    setup {\n        run(\"ABRICATE_RUN\") {\n            script \"../../run/main.nf\"\n            process {\n                \"\"\"\n                input[0] =  Channel.fromList([\n                    tuple([ id:'test1', single_end:false ], // meta map\n                        file(params.test_data['bacteroides_fragilis']['genome']['genome_fna_gz'], checkIfExists: true)),\n                    tuple([ id:'test2', single_end:false ],\n                        file(params.test_data['haemophilus_influenzae']['genome']['genome_fna_gz'], checkIfExists: true))\n                ])\n                \"\"\"\n            }\n        }\n    }\n\n    test(\"first test\") {\n        when {\n            process {\n                \"\"\"\n                input[0] = ABRICATE_RUN.out.report.collect{ meta, report -&gt; report }.map{ report -&gt; [[ id: 'test_summary'], report]}\n                \"\"\"\n            }\n        }\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n    }\n\n    test(\"second test\") {\n        when {\n            process {\n                \"\"\"\n                input[0] = ABRICATE_RUN.out.report.collect{ meta, report -&gt; report }.map{ report -&gt; [[ id: 'test_summary'], report]}\n                \"\"\"\n            }\n        }\n        then {\n            assert process.success\n            assert snapshot(process.out).match()\n        }\n    }\n\n}\n</code></pre>"},{"location":"docs/testcases/setup/#3-aliasing-of-dependencies","title":"3. Aliasing of Dependencies","text":"<p>In this example, the process <code>UNTAR</code> is used multiple times in the setup method:</p> <pre><code>nextflow_process {\n\n    ...\n\n    setup {\n\n        run(\"UNTAR\", alias: \"UNTAR1\") {\n            script \"modules/nf-core/untar/main.nf\"\n            process {\n            \"\"\"\n            input[0] = Channel.fromList(...)\n            \"\"\"\n            }\n        }\n\n        run(\"UNTAR\", alias: \"UNTAR2\") {\n            script \"modules/nf-core/untar/main.nf\"\n            process {\n            \"\"\"\n            input[0] = Channel.fromList(...)\n            \"\"\"\n            }\n        }\n\n        run(\"UNTAR\", alias: \"UNTAR3\") {\n            script \"modules/nf-core/untar/main.nf\"\n            process {\n            \"\"\"\n            input[0] = Channel.fromList(...)\n            \"\"\"\n            }\n        }\n    }\n\n    test(\"Test with three different inputs\") {\n        when {\n            process {\n                \"\"\"\n                input[0] = UNTAR1.out.untar.map{ it[1] }\n                input[1] = UNTAR2.out.untar.map{ it[1] }\n                input[2] = UNTAR3.out.untar.map{ it[1] }\n                \"\"\"\n            }\n        }\n\n        then {\n            ...\n        }\n\n }\n\n}\n</code></pre>"},{"location":"tutorials/github-actions/","title":"Setup nf-test on GitHub Actions","text":"<p>Warning</p> <p>This feature requires nf-test 0.9.0 or higher</p> <p>In this tutorial, we will guide you through setting up and running <code>nf-test</code> on GitHub Actions. We will start with a simple example where all tests run in a single job, then extend it to demonstrate how you can use sharding to distribute tests across multiple jobs for improved efficiency. Finally, we will show you how to run only the tests affected by the changed files using the <code>--changes-since</code> option.</p> <p>By the end of this tutorial, you will have a clear understanding of how to:</p> <ol> <li>Set up a basic CI workflow for running <code>nf-test</code> on GitHub Actions.</li> <li>Extend the workflow to use sharding, allowing tests to run in parallel, which can significantly reduce the overall execution time.</li> <li>Configure the workflow to run only the test cases affected by the changed files, optimizing the CI process further.</li> </ol> <p>Whether you are maintaining a complex bioinformatics pipeline or a simple data analysis workflow, integrating <code>nf-test</code> with GitHub Actions will help ensure the robustness and reliability of your code. Let's get started!</p>"},{"location":"tutorials/github-actions/#step-1-running-nf-test","title":"Step 1: Running nf-test","text":"<p>Create a file named <code>.github/workflows/ci-tests.yml</code> in your repository with the following content:</p> <pre><code>name: CI Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Set up JDK 11\n        uses: actions/setup-java@v2\n        with:\n          java-version: '11'\n          distribution: 'adopt'\n\n      - name: Setup Nextflow latest-edge\n        uses: nf-core/setup-nextflow@v1\n        with:\n          version: \"latest-edge\"\n\n      - name: Install nf-test\n        run: |\n          wget -qO- https://get.nf-test.com | bash\n          sudo mv nf-test /usr/local/bin/\n\n      - name: Run Tests\n        run: nf-test test --ci\n</code></pre>"},{"location":"tutorials/github-actions/#explanation","title":"Explanation:","text":"<ol> <li>Checkout: Uses the <code>actions/checkout@v2</code> action to check out the repository.</li> <li>Set up JDK 11: Uses the <code>actions/setup-java@v2</code> action to set up Java Development Kit version 11.</li> <li>Setup Nextflow: Uses the <code>nf-core/setup-nextflow@v1</code> action to install the latest-edge version of Nextflow.</li> <li>Install nf-test: Downloads and installs nf-test.</li> <li>Run Tests: Runs nf-test with the <code>--ci</code> flag. This activates the CI mode. Instead of automatically storing a new snapshot as per usual, it will now fail the test if no reference snapshot is available. This enables tests to fail when a snapshot file was forgotten to be committed.</li> </ol>"},{"location":"tutorials/github-actions/#step-2-extending-to-use-sharding","title":"Step 2: Extending to Use Sharding","text":"<p>To distribute the tests across multiple jobs, you can set up sharding. Update your workflow file as follows:</p> <pre><code>name: CI Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        shard: [1, 2, 3, 4]\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Set up JDK 11\n        uses: actions/setup-java@v2\n        with:\n          java-version: '11'\n          distribution: 'adopt'\n\n      - name: Setup Nextflow latest-edge\n        uses: nf-core/setup-nextflow@v1\n        with:\n          version: \"latest-edge\"\n\n      - name: Install nf-test\n        run: |\n          wget -qO- https://get.nf-test.com | bash\n          sudo mv nf-test /usr/local/bin/\n\n      - name: Run Tests (Shard ${{ matrix.shard }}/${{ strategy.job-total }})\n        run: nf-test test --ci --shard ${{ matrix.shard }}/${{ strategy.job-total }}\n</code></pre>"},{"location":"tutorials/github-actions/#explanation-of-sharding","title":"Explanation of Sharding:","text":"<ol> <li>Matrix Strategy: The <code>strategy</code> section defines a matrix with a <code>shard</code> parameter that has four values: <code>[1, 2, 3, 4]</code>. This will create four parallel jobs, one for each shard.</li> <li>Run Tests with Sharding: The <code>run</code> command for running tests is updated to <code>nf-test test --shard ${{ matrix.shard }}/${{ strategy.job-total }}</code>. This command will run the tests for the specific shard. <code>${{ matrix.shard }}</code> represents the current shard number, and <code>${{ strategy.job-total }}</code> represents the total number of shards.</li> </ol>"},{"location":"tutorials/github-actions/#step-3-running-only-tests-affected-by-changed-files","title":"Step 3: Running Only Tests Affected by Changed Files","text":"<p>To optimize the workflow further, you can run only the tests that are affected by the changed files since the last commit. Update your workflow file as follows:</p> <pre><code>name: CI Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        shard: [1, 2, 3, 4]\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Set up JDK 11\n        uses: actions/setup-java@v2\n        with:\n          java-version: '11'\n          distribution: 'adopt'\n\n      - name: Setup Nextflow latest-edge\n        uses: nf-core/setup-nextflow@v1\n        with:\n          version: \"latest-edge\"\n\n      - name: Install nf-test\n        run: |\n          wget -qO- https://get.nf-test.com | bash\n          sudo mv nf-test /usr/local/bin/\n\n      - name: Run Tests (Shard ${{ matrix.shard }}/${{ strategy.job-total }})\n        run: nf-test test --ci --shard ${{ matrix.shard }}/${{ strategy.job-total }} --changed-since HEAD^\n</code></pre>"},{"location":"tutorials/github-actions/#explanation-of-changes","title":"Explanation of Changes:","text":"<ol> <li>Checkout with Full History: The <code>actions/checkout@v2</code> action is updated with <code>fetch-depth: 0</code> to fetch the full history of the repository. This is necessary for accurately determining the changes since the last commit.</li> <li>Run Tests with Changed Files: The <code>run</code> command is further updated to include the <code>--changed-since HEAD^</code> option. This option ensures that only the tests affected by the changes since the previous commit are run.</li> </ol>"},{"location":"tutorials/github-actions/#step-4-adapting-nf-testconfig-to-trigger-full-test-runs","title":"Step 4: Adapting nf-test.config to Trigger Full Test Runs","text":"<p>In some cases, changes to specific critical files should trigger a full test run, regardless of other changes. To configure this, you need to adapt your <code>nf-test.config</code> file.</p> <p>Add the following lines to your <code>nf-test.config</code>:</p> <pre><code>config {\n    ...\n    triggers 'nextflow.config', 'nf-test.config', 'test-data/**/*'\n    ...\n}\n</code></pre> <p>The <code>triggers</code> directive in <code>nf-test.config</code> specifies a list of filenames or patterns that should trigger a full test run. For example:</p> <pre><code>- `'nextflow.config'`: Changes to the main Nextflow configuration file will trigger a full test run.\n- `'nf-test.config'`: Changes to the nf-test configuration file itself will trigger a full test run.\n- `'test-data/**/*'`: Changes to any files within the `test-data` directory will trigger a full test run.\n</code></pre> <p>This configuration ensures that critical changes always result in a comprehensive validation of the pipeline, providing additional confidence in your CI process.</p>"},{"location":"tutorials/github-actions/#step-5-additional-useful-options","title":"Step 5: Additional useful Options","text":"<p>The <code>--filter</code> flag allows you to selectively run test cases based on their specified types. For example, you can filter tests by process, pipeline, workflow, or function. This is particularly useful when you have a large suite of tests and need to focus on specific areas of functionality. By separating multiple types with commas, you can run a customized subset of tests that match the exact criteria you're interested in, thereby saving time and resources.</p> <p>The <code>--related-tests</code> flag enables you to identify and execute all tests related to the provided <code>.nf</code> or <code>nf.test</code> files. This is ideal for scenarios where you have made changes to specific files and want to ensure that only the relevant tests are run. You can provide multiple files by separating them with spaces, which makes it easy to manage and test multiple changes at once, ensuring thorough validation of your updates.</p> <p>When the <code>--follow-dependencies</code> flag is set, the nf-test tool will automatically traverse and execute all tests for dependencies related to the files specified with the <code>--related-tests</code> flag. This ensures that any interdependent components are also tested, providing comprehensive coverage. This option is particularly useful for complex projects with multiple dependencies, as it bypasses the firewall calculation process and guarantees that all necessary tests are executed.</p> <p>The <code>--changed-until</code> flag allows you to run tests based on changes made up until a specified commit hash or branch name. By default, this parameter uses <code>HEAD</code>, but you can specify any commit or branch to target the changes made up to that point. This is particularly useful for validating changes over a specific range of commits, ensuring that all modifications within that period are tested comprehensively.</p>"},{"location":"tutorials/github-actions/#summary","title":"Summary","text":"<ol> <li>Without Sharding: A straightforward setup where all tests run in a single job.</li> <li>With Sharding: Distributes tests across multiple jobs, allowing them to run in parallel.</li> <li>With Sharding and Changed Files: Optimizes the CI process by running only the tests affected by the changed files since the last commit, in parallel jobs.</li> </ol> <p>Choose the configuration that best suits your project's needs. Start with the simpler setup and extend it as needed to improve efficiency and reduce test execution time.</p>"}]}